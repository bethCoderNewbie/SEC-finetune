# -*- coding: utf-8 -*-
"""M10.2 - financial_analysis_finetuning.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1rwNhBKJG9w6-Bs-i-uvdlu8vXLrQ7SH0

# M10.3 - Fine-tuning for Financial Analysis

## Learning Objectives
- Fine-tune models for financial sentiment analysis
- Work with financial domain-specific data
- Generate insights from earnings call transcripts
- Evaluate models on financial metrics
- Build a financial analysis assistant

## Business Context

**Use Cases**:
1. **Automated Sentiment Analysis**: Analyze thousands of financial news articles, earnings calls, and reports
2. **Earnings Call Summarization**: Extract key insights from hour-long transcripts
3. **Risk Detection**: Identify cautionary language and risk factors
4. **Investment Research**: Automatically categorize and prioritize financial information

**Why Fine-tune for Finance?**
- Financial language is specialized ("beat expectations", "guidance", "headwinds")
- Sentiment is nuanced ("cautiously optimistic" vs "confident")
- Context matters greatly
- Pre-trained models don't understand financial semantics

**Value Proposition**:
- Process earnings calls in minutes vs hours
- Consistent, unbiased analysis
- Scale to thousands of companies
- Identify trends and patterns

---

## Part 1: Setup and Dependencies
"""

# Install required packages
# !pip install -q transformers datasets peft bitsandbytes accelerate trl
# !pip install -q pandas numpy matplotlib seaborn scikit-learn
# !pip install -q yfinance  # For downloading financial data
# !pip install -q openai  # For synthetic data generation (optional)

import torch
import pandas as pd
import numpy as np
import json
import matplotlib.pyplot as plt
import seaborn as sns
from collections import Counter
from tqdm.auto import tqdm

from datasets import load_dataset, Dataset
from transformers import (
    AutoTokenizer,
    AutoModelForSequenceClassification,
    AutoModelForCausalLM,
    TrainingArguments,
    Trainer,
    BitsAndBytesConfig
)
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training

import warnings
warnings.filterwarnings('ignore')

sns.set_style('whitegrid')
plt.rcParams['figure.figsize'] = (12, 6)

print(f"PyTorch version: {torch.__version__}")
print(f"CUDA available: {torch.cuda.is_available()}")
if torch.cuda.is_available():
    print(f"CUDA device: {torch.cuda.get_device_name(0)}")

"""## Part 2: Load Financial PhraseBank Dataset

**Financial PhraseBank** contains ~5,000 sentences from financial news categorized by sentiment:
- **Positive**: Company performance exceeds expectations, growth, positive outlook
- **Negative**: Losses, declines, warnings, restructuring
- **Neutral**: Factual statements without clear sentiment

This dataset is annotated by financial experts, making it gold standard for financial sentiment.
"""

# Load Financial PhraseBank
print("Loading Financial PhraseBank dataset...")

dataset = load_dataset("financial_phrasebank", "sentences_allagree", trust_remote_code=True)

print("\nDataset loaded!")
print(dataset)

# Examine structure
print("\nFirst few examples:")
for i in range(3):
    example = dataset['train'][i]
    print(f"\n{i+1}. Sentence: {example['sentence']}")
    print(f"   Label: {example['label']} (0=negative, 1=neutral, 2=positive)")

# Analyze dataset
label_names = ['Negative', 'Neutral', 'Positive']
df = pd.DataFrame(dataset['train'])
df['sentiment'] = df['label'].map({0: 'Negative', 1: 'Neutral', 2: 'Positive'})

print("\nDataset Statistics:")
print("="*80)
print(f"Total examples: {len(df)}")
print(f"\nLabel distribution:")
print(df['sentiment'].value_counts())
print(f"\nPercentages:")
print(df['sentiment'].value_counts(normalize=True) * 100)

# Visualize
fig, axes = plt.subplots(1, 2, figsize=(14, 5))

# Class distribution
df['sentiment'].value_counts().plot(kind='bar', ax=axes[0], color=['#d9534f', '#5bc0de', '#5cb85c'])
axes[0].set_title('Sentiment Distribution', fontsize=14, fontweight='bold')
axes[0].set_xlabel('Sentiment')
axes[0].set_ylabel('Count')
axes[0].set_xticklabels(axes[0].get_xticklabels(), rotation=0)

# Sentence length distribution
df['length'] = df['sentence'].str.split().str.len()
axes[1].hist(df['length'], bins=30, edgecolor='black', alpha=0.7)
axes[1].set_title('Sentence Length Distribution', fontsize=14, fontweight='bold')
axes[1].set_xlabel('Word Count')
axes[1].set_ylabel('Frequency')
axes[1].axvline(df['length'].mean(), color='red', linestyle='--', label=f'Mean: {df["length"].mean():.1f}')
axes[1].legend()

plt.tight_layout()
plt.savefig('financial_phrasebank_analysis.png', dpi=150, bbox_inches='tight')
plt.show()

"""### Example Sentences by Sentiment"""

print("\nExample sentences by sentiment:")
print("="*80)

for label_id, label_name in enumerate(label_names):
    examples = df[df['label'] == label_id].sample(3)
    print(f"\n{label_name.upper()}:")
    print("-"*80)
    for idx, row in examples.iterrows():
        print(f"  â€¢ {row['sentence']}")
    print()

"""## Part 3: Prepare Data for Training"""

from sklearn.model_selection import train_test_split

# Split into train/validation/test
# Financial PhraseBank only has 'train' split, so we'll create our own
train_val_df, test_df = train_test_split(df, test_size=0.15, stratify=df['label'], random_state=42)
train_df, val_df = train_test_split(train_val_df, test_size=0.15, stratify=train_val_df['label'], random_state=42)

print(f"Dataset splits:")
print(f"  Training:   {len(train_df)} ({len(train_df)/len(df)*100:.1f}%)")
print(f"  Validation: {len(val_df)} ({len(val_df)/len(df)*100:.1f}%)")
print(f"  Test:       {len(test_df)} ({len(test_df)/len(df)*100:.1f}%)")

# Convert to HuggingFace datasets
train_dataset = Dataset.from_pandas(train_df[['sentence', 'label']].reset_index(drop=True))
val_dataset = Dataset.from_pandas(val_df[['sentence', 'label']].reset_index(drop=True))
test_dataset = Dataset.from_pandas(test_df[['sentence', 'label']].reset_index(drop=True))

print("\nâœ“ Datasets created and ready for fine-tuning")

"""## Part 4: Fine-tune FinBERT (Option 1: Domain-Specific Model)

**FinBERT** is a BERT model pre-trained on financial text. It already understands financial language, so fine-tuning should be very effective.
"""

# Load FinBERT
model_name = "ProsusAI/finbert"
num_labels = 3

print(f"Loading {model_name}...")

tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForSequenceClassification.from_pretrained(
    model_name,
    num_labels=num_labels,
    ignore_mismatched_sizes=True  # FinBERT might have different head
)

print("âœ“ Model loaded successfully")

# Tokenize datasets
def tokenize_function(examples):
    return tokenizer(
        examples['sentence'],
        padding='max_length',
        truncation=True,
        max_length=128
    )

print("Tokenizing datasets...")
train_dataset = train_dataset.map(tokenize_function, batched=True)
val_dataset = val_dataset.map(tokenize_function, batched=True)
test_dataset = test_dataset.map(tokenize_function, batched=True)

# Set format
train_dataset.set_format('torch', columns=['input_ids', 'attention_mask', 'label'])
val_dataset.set_format('torch', columns=['input_ids', 'attention_mask', 'label'])
test_dataset.set_format('torch', columns=['input_ids', 'attention_mask', 'label'])

print("âœ“ Tokenization complete")

# Define metrics
def compute_metrics(eval_pred):
    predictions, labels = eval_pred
    predictions = np.argmax(predictions, axis=1)
    accuracy = accuracy_score(labels, predictions)

    # Per-class accuracy (important for imbalanced datasets)
    report = classification_report(labels, predictions, output_dict=True, zero_division=0)

    return {
        'accuracy': accuracy,
        'f1_macro': report['macro avg']['f1-score'],
        'f1_weighted': report['weighted avg']['f1-score']
    }

# Training arguments
training_args = TrainingArguments(
    output_dir="./finbert_finetuned",
    num_train_epochs=4,
    per_device_train_batch_size=16,
    per_device_eval_batch_size=16,
    learning_rate=2e-5,
    warmup_steps=100,
    weight_decay=0.01,
    logging_steps=50,
    evaluation_strategy="epoch",
    save_strategy="epoch",
    load_best_model_at_end=True,
    metric_for_best_model="f1_macro",
    report_to="none"
)

# Create trainer
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=val_dataset,
    compute_metrics=compute_metrics
)

print("âœ“ Trainer configured")

# Train
print("Starting fine-tuning...\n")
trainer.train()
print("\nâœ“ Fine-tuning complete!")

"""## Part 5: Evaluation"""

# Evaluate on test set
print("Evaluating on test set...\n")

predictions = trainer.predict(test_dataset)
pred_labels = np.argmax(predictions.predictions, axis=1)
true_labels = predictions.label_ids

# Calculate metrics
accuracy = accuracy_score(true_labels, pred_labels)

print("="*80)
print("FINANCIAL SENTIMENT ANALYSIS - TEST SET RESULTS")
print("="*80)
print(f"Accuracy: {accuracy:.2%}\n")
print("Classification Report:")
print(classification_report(
    true_labels,
    pred_labels,
    target_names=label_names,
    digits=3
))

# Confusion matrix
cm = confusion_matrix(true_labels, pred_labels)
plt.figure(figsize=(8, 6))
sns.heatmap(
    cm,
    annot=True,
    fmt='d',
    cmap='Blues',
    xticklabels=label_names,
    yticklabels=label_names,
    cbar_kws={'label': 'Count'}
)
plt.title('Confusion Matrix - Financial Sentiment', fontsize=14, fontweight='bold', pad=20)
plt.xlabel('Predicted Sentiment', fontsize=12)
plt.ylabel('True Sentiment', fontsize=12)
plt.tight_layout()
plt.savefig('financial_sentiment_confusion_matrix.png', dpi=150, bbox_inches='tight')
plt.show()

"""### Test on Real Financial News"""

def analyze_financial_text(text, model, tokenizer):
    """
    Analyze sentiment of financial text
    """
    inputs = tokenizer(text, return_tensors="pt", padding=True, truncation=True, max_length=128)
    inputs = {k: v.to(model.device) for k, v in inputs.items()}

    with torch.no_grad():
        outputs = model(**inputs)

    probs = torch.nn.functional.softmax(outputs.logits, dim=-1)
    pred_id = torch.argmax(probs, dim=-1).item()
    confidence = probs[0, pred_id].item()

    sentiment = label_names[pred_id]

    return sentiment, confidence, probs[0].cpu().numpy()

# Test examples (create realistic financial news)
test_financial_news = [
    "Apple reports record quarterly revenue, beating analyst expectations by 15%.",
    "Tesla shares plunge 20% after missing delivery targets and cutting guidance.",
    "The company announced a restructuring plan to reduce operating costs.",
    "Strong demand for AI chips drives NVIDIA's profit up 200% year-over-year.",
    "The Federal Reserve maintained interest rates at current levels.",
    "Amazon faces regulatory scrutiny over antitrust concerns in European markets.",
    "Microsoft cloud revenue growth accelerates, exceeding Wall Street estimates.",
    "Boeing delays aircraft deliveries due to supply chain disruptions.",
    "The merger is expected to close in Q3 pending regulatory approval.",
    "Pfizer's new drug shows promising results in Phase 3 clinical trials."
]

print("\n" + "="*80)
print("ANALYZING REAL FINANCIAL NEWS")
print("="*80)

results = []
for text in test_financial_news:
    sentiment, confidence, probs = analyze_financial_text(text, model, tokenizer)
    results.append({
        'text': text,
        'sentiment': sentiment,
        'confidence': confidence,
        'neg_prob': probs[0],
        'neu_prob': probs[1],
        'pos_prob': probs[2]
    })

    # Color code sentiment
    color = {'Positive': '\033[92m', 'Negative': '\033[91m', 'Neutral': '\033[94m'}[sentiment]
    reset = '\033[0m'

    print(f"\n{text}")
    print(f"  â†’ {color}{sentiment}{reset} ({confidence:.1%} confidence)")
    print(f"     [Neg: {probs[0]:.2f} | Neu: {probs[1]:.2f} | Pos: {probs[2]:.2f}]")

# Convert to DataFrame for analysis
results_df = pd.DataFrame(results)
print(f"\n{'-'*80}")
print(f"Summary: {(results_df['sentiment']=='Positive').sum()} Positive, "
      f"{(results_df['sentiment']=='Negative').sum()} Negative, "
      f"{(results_df['sentiment']=='Neutral').sum()} Neutral")
print(f"Average Confidence: {results_df['confidence'].mean():.1%}")

"""## Part 6: Earnings Call Analysis (Optional Advanced Exercise)

Let's create a more advanced application: analyzing earnings call transcripts to extract key insights.
"""

# Sample earnings call excerpt (you could load real transcripts)
earnings_call_excerpt = """
Thank you for joining us today. I'm pleased to report that Q4 was another strong quarter for our company.
Revenue increased 25% year-over-year to $5.2 billion, exceeding our guidance range.
Operating margins expanded by 200 basis points due to operational efficiencies and favorable product mix.
We're seeing strong customer adoption of our cloud platform, with over 50,000 new enterprise customers added this quarter.

However, I must note that we're facing some headwinds in the European market due to macroeconomic uncertainty.
We've also seen increased competition in the mid-market segment, which has pressured pricing.
Despite these challenges, we remain optimistic about our long-term growth prospects.

For Q1, we expect revenue between $5.0 and $5.3 billion, representing 15-20% year-over-year growth.
We're investing heavily in R&D to maintain our competitive advantage, particularly in AI and machine learning.
Our balance sheet remains strong with $2.5 billion in cash and no debt.
"""

# Split into sentences
import re
sentences = [s.strip() for s in re.split(r'[.!?]+', earnings_call_excerpt) if s.strip()]

print("\n" + "="*80)
print("EARNINGS CALL SENTIMENT ANALYSIS")
print("="*80)

call_results = []
for sentence in sentences:
    if len(sentence.split()) > 5:  # Skip very short sentences
        sentiment, confidence, probs = analyze_financial_text(sentence, model, tokenizer)
        call_results.append({
            'sentence': sentence,
            'sentiment': sentiment,
            'confidence': confidence,
            'sentiment_score': probs[2] - probs[0]  # Positive - Negative
        })

call_df = pd.DataFrame(call_results)

# Display results
for idx, row in call_df.iterrows():
    color = {'Positive': '\033[92m', 'Negative': '\033[91m', 'Neutral': '\033[94m'}[row['sentiment']]
    reset = '\033[0m'
    print(f"\n{color}[{row['sentiment']}]{reset} {row['sentence']}")

# Overall sentiment
print(f"\n{'-'*80}")
print("\nOVERALL EARNINGS CALL SENTIMENT:")
print(f"  Positive statements: {(call_df['sentiment']=='Positive').sum()}")
print(f"  Negative statements: {(call_df['sentiment']=='Negative').sum()}")
print(f"  Neutral statements:  {(call_df['sentiment']=='Neutral').sum()}")
print(f"  Average sentiment score: {call_df['sentiment_score'].mean():.2f} (range: -1 to +1)")

if call_df['sentiment_score'].mean() > 0.2:
    print("  \n  ðŸ“ˆ Overall POSITIVE tone")
elif call_df['sentiment_score'].mean() < -0.2:
    print("  \n  ðŸ“‰ Overall NEGATIVE tone")
else:
    print("  \n  âž¡ï¸  Overall NEUTRAL tone")

# Visualize sentiment distribution in earnings call
fig, axes = plt.subplots(1, 2, figsize=(14, 5))

# Sentiment distribution
call_df['sentiment'].value_counts().plot(
    kind='bar',
    ax=axes[0],
    color=['#d9534f', '#5bc0de', '#5cb85c']
)
axes[0].set_title('Earnings Call Sentiment Distribution', fontsize=14, fontweight='bold')
axes[0].set_xlabel('Sentiment')
axes[0].set_ylabel('Count')
axes[0].set_xticklabels(axes[0].get_xticklabels(), rotation=0)

# Sentiment score distribution
axes[1].hist(call_df['sentiment_score'], bins=15, edgecolor='black', alpha=0.7)
axes[1].axvline(call_df['sentiment_score'].mean(), color='red', linestyle='--',
                label=f'Mean: {call_df["sentiment_score"].mean():.2f}')
axes[1].axvline(0, color='gray', linestyle='-', alpha=0.3)
axes[1].set_title('Sentiment Score Distribution', fontsize=14, fontweight='bold')
axes[1].set_xlabel('Sentiment Score (Positive - Negative)')
axes[1].set_ylabel('Frequency')
axes[1].legend()

plt.tight_layout()
plt.savefig('earnings_call_sentiment.png', dpi=150, bbox_inches='tight')
plt.show()

"""## Part 7: Save Model"""

# Save fine-tuned model
output_dir = "./finbert_financial_sentiment"
model.save_pretrained(output_dir)
tokenizer.save_pretrained(output_dir)

# Save label mapping
with open(f"{output_dir}/label_mapping.json", 'w') as f:
    json.dump({
        'labels': label_names,
        'id2label': {i: label for i, label in enumerate(label_names)},
        'label2id': {label: i for i, label in enumerate(label_names)}
    }, f, indent=2)

print(f"âœ“ Model saved to: {output_dir}")

"""## Part 8: Business Applications

### Application Ideas:

1. **Real-time News Monitoring**
   - Monitor financial news APIs (Bloomberg, Reuters)
   - Classify sentiment in real-time
   - Alert on significant negative sentiment

2. **Portfolio Risk Assessment**
   - Analyze sentiment for all holdings
   - Track sentiment trends over time
   - Flag companies with deteriorating sentiment

3. **Earnings Call Dashboard**
   - Automatically analyze transcripts
   - Compare sentiment across quarters
   - Identify key topics (risks, opportunities)

4. **Competitive Intelligence**
   - Track sentiment for competitors
   - Identify market trends
   - Monitor regulatory/legal issues

5. **Investment Research Automation**
   - Process thousands of analyst reports
   - Summarize consensus sentiment
   - Identify contrarian signals

## Key Takeaways

### What You Learned:

1. **Domain-Specific Fine-tuning**: FinBERT's pre-training on financial text provides a strong foundation
2. **Financial Sentiment**: More nuanced than general sentiment (cautiously optimistic, headwinds, etc.)
3. **Practical Applications**: From news monitoring to earnings analysis
4. **Evaluation**: Financial sentiment requires domain expertise for validation
5. **Business Impact**: Automate analysis of thousands of documents

### Business Value:

- **Time Savings**: Analyze hours of earnings calls in minutes
- **Consistency**: Unbiased, repeatable analysis
- **Scale**: Process thousands of companies simultaneously
- **Insights**: Identify trends and patterns humans might miss
- **Speed**: Real-time sentiment for trading/investment decisions

### Model Accuracy:

- **Financial PhraseBank**: Typically 85-92% accuracy
- **Real-world performance**: Varies by domain and data quality
- **FinBERT vs General BERT**: 5-10% accuracy improvement on financial text

---

## Exercises

### Exercise 1: Multi-class Financial Classification
Extend beyond sentiment to classify financial topics:
- Risk factors
- Growth opportunities
- Regulatory issues
- Competitive threats
- Market conditions

### Exercise 2: Earnings Call Summarization
Fine-tune an LLM (Llama, Mistral) to:
- Summarize key points from earnings calls
- Extract financial metrics (revenue, margins, guidance)
- Identify management tone and confidence

### Exercise 3: Financial News Aggregation
Build a system that:
- Fetches news from multiple sources (API integration)
- Classifies sentiment
- Aggregates sentiment by company/sector
- Tracks sentiment trends over time
- Generates daily sentiment reports

### Exercise 4: Compare Models
Fine-tune and compare:
- DistilBERT (general purpose)
- FinBERT (financial specific)
- RoBERTa (more powerful but slower)
- Domain-adapted models

Evaluate on: accuracy, speed, robustness

### Exercise 5: Real-time Trading Signal
Build an advanced system:
- Monitor news APIs in real-time
- Classify sentiment with fine-tuned model
- Weight by source credibility
- Generate trading signals based on sentiment shifts
- Backtest strategy performance

### Exercise 6: SEC Filing Analysis
Analyze 10-K and 10-Q filings:
- Extract Risk Factors section
- Classify risk severity
- Compare risk disclosure across companies
- Track changes in risk language over time

---
"""