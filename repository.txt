# SEC Filing Analyzer - Repository Structure
# Updated: 2025-01-17
#
# GUIDANCE:
# - All code must use Pydantic v2.12.4+ (enforced by validation script)
# - Configuration uses YAML files in configs/ with Pydantic Settings
# - Scripts organized by ML lifecycle stages (01-10)
# - Use pyproject.toml for dependency management (no requirements.txt)
# - Run validation before commits: python scripts/utils/validation/validate_pydantic_v2.py

sec-filing-analyzer/
â”‚
â”œâ”€â”€ .git/                   # Git version control
â”œâ”€â”€ .gitignore              # Ignores data/, logs/, venv/, model checkpoints
â”‚
â”œâ”€â”€ README.md               # Project overview, setup, usage
â”œâ”€â”€ pyproject.toml          # Project metadata, dependencies (Pydantic >=2.12.4)
â”œâ”€â”€ repository.txt          # This file - repository structure reference
â”‚
â”œâ”€â”€ configs/                # YAML configuration files (Pydantic Settings)
â”‚   â”œâ”€â”€ README.md           # Configuration system documentation
â”‚   â”œâ”€â”€ config.yaml         # Main application configuration
â”‚   â””â”€â”€ features/
â”‚       â”œâ”€â”€ sentiment.yaml  # Sentiment analysis configuration
â”‚       â””â”€â”€ readability.yaml # Readability/complexity analysis configuration
â”‚
â”œâ”€â”€ data/                   # Data directory (versioned by DVC, not in Git)
â”‚   â”œâ”€â”€ dictionary/         # Reference dictionaries (e.g., Loughran-McDonald)
â”‚   â”œâ”€â”€ raw/                # Original SEC filings (HTML/XBRL)
â”‚   â”œâ”€â”€ interim/            # Intermediate processed data
â”‚   â”‚   â”œâ”€â”€ parsed/         # Parsed SEC filings (JSON)
â”‚   â”‚   â””â”€â”€ extracted/      # Extracted sections
â”‚   â””â”€â”€ processed/          # Final datasets ready for modeling
â”‚       â””â”€â”€ features/       # Extracted features (sentiment, readability, embeddings)
â”‚
â”œâ”€â”€ docs/                   # Project documentation
â”‚   â”œâ”€â”€ README.md           # Documentation index with Pydantic v2 quick start
â”‚   â”œâ”€â”€ PYDANTIC_V2_ENFORCEMENT.md  # âš ï¸ REQUIRED: Pydantic v2 enforcement guide
â”‚   â”œâ”€â”€ ENUM_CONFIG_PATTERNS.md     # Configuration patterns (Pydantic v2)
â”‚   â”œâ”€â”€ CONFIG_MIGRATION_GUIDE.md   # Configuration migration guide
â”‚   â””â”€â”€ requirements/       # Historical requirements (reference only)
â”‚
â”œâ”€â”€ examples/               # Example scripts demonstrating usage
â”‚   â”œâ”€â”€ 01_basic_extraction.py              # Basic section extraction
â”‚   â”œâ”€â”€ 02_complete_pipeline.py             # Full pipeline example
â”‚   â”œâ”€â”€ 03_sentiment_analysis.py            # Sentiment analysis example
â”‚   â””â”€â”€ 04_sentiment_risk_classification.py # Risk classification example
â”‚
â”œâ”€â”€ logs/                   # Application logs (not in Git)
â”‚   â””â”€â”€ extractions/        # Extraction process logs
â”‚
â”œâ”€â”€ models/                 # Trained models (tracked by Git LFS or DVC)
â”‚   â””â”€â”€ README.md           # Model storage and versioning info
â”‚
â”œâ”€â”€ reports/                # Generated analysis and visualizations
â”‚
â”œâ”€â”€ scripts/                # ML lifecycle scripts (numbered by stage)
â”‚   â”œâ”€â”€ README.md           # Scripts directory overview
â”‚   â”‚
â”‚   â”œâ”€â”€ 01_data_collection/         # Stage 1: Data acquisition
â”‚   â”‚   â””â”€â”€ download_sec_filings.py
â”‚   â”‚
â”‚   â”œâ”€â”€ 02_data_preprocessing/      # Stage 2: Parsing and extraction
â”‚   â”‚   â”œâ”€â”€ batch_parse.py
â”‚   â”‚   â””â”€â”€ run_preprocessing_pipeline.py
â”‚   â”‚
â”‚   â”œâ”€â”€ 03_eda/                     # Stage 3: Exploratory data analysis
â”‚   â”‚   â””â”€â”€ exploratory_analysis.py
â”‚   â”‚
â”‚   â”œâ”€â”€ 04_feature_engineering/     # Stage 4: Feature extraction
â”‚   â”‚   â”œâ”€â”€ extract_features.py
â”‚   â”‚   â””â”€â”€ utils/
â”‚   â”‚       â””â”€â”€ preprocess_lm_dict.py
â”‚   â”‚
â”‚   â”œâ”€â”€ 05_data_splitting/          # Stage 5: Train/test splits
â”‚   â”‚   â””â”€â”€ create_train_test_split.py
â”‚   â”‚
â”‚   â”œâ”€â”€ 06_training/                # Stage 6: Model training
â”‚   â”‚   â””â”€â”€ train_model.py
â”‚   â”‚
â”‚   â”œâ”€â”€ 07_evaluation/              # Stage 7: Model evaluation
â”‚   â”‚   â””â”€â”€ evaluate_model.py
â”‚   â”‚
â”‚   â”œâ”€â”€ 08_inference/               # Stage 8: Predictions
â”‚   â”‚   â””â”€â”€ predict.py
â”‚   â”‚
â”‚   â”œâ”€â”€ 09_deployment/              # Stage 9: Deployment (placeholder)
â”‚   â”œâ”€â”€ 10_monitoring/              # Stage 10: Monitoring (placeholder)
â”‚   â”‚
â”‚   â””â”€â”€ utils/                      # Development utilities
â”‚       â”œâ”€â”€ README.md               # Utilities documentation
â”‚       â”œâ”€â”€ check_installation.py  # Verify environment setup
â”‚       â”œâ”€â”€ setup_nlp_models.py    # Download NLP models
â”‚       â”œâ”€â”€ show_segment_sample.py # Display data samples
â”‚       â”‚
â”‚       â”œâ”€â”€ debugging/              # Debug tools
â”‚       â”‚   â”œâ”€â”€ diagnose_extraction.py
â”‚       â”‚   â””â”€â”€ debug_node_structure.py
â”‚       â”‚
â”‚       â”œâ”€â”€ inspection/             # Data inspection tools
â”‚       â”‚   â””â”€â”€ inspect_parsed.py
â”‚       â”‚
â”‚       â”œâ”€â”€ testing/                # Testing utilities
â”‚       â”‚   â””â”€â”€ test_extractor_fix.py
â”‚       â”‚
â”‚       â””â”€â”€ validation/             # âš ï¸ Code quality validation
â”‚           â”œâ”€â”€ README.md
â”‚           â””â”€â”€ validate_pydantic_v2.py  # Pydantic v2 enforcement
â”‚
â”œâ”€â”€ src/                    # Main source code (importable Python package)
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚
â”‚   â”œâ”€â”€ config.py           # âš ï¸ Global configuration (Pydantic Settings + YAML)
â”‚   â”‚                       # 21 Pydantic v2 models (incl. ReadabilityConfig)
â”‚   â”‚
â”‚   â”œâ”€â”€ preprocessing/      # SEC filing parsing and extraction
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ parser.py       # Parse HTML/XBRL filings (sec-parser)
â”‚   â”‚   â””â”€â”€ extractor.py    # Extract specific sections (Item 1A, etc.)
â”‚   â”‚
â”‚   â”œâ”€â”€ features/           # Feature engineering modules
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ sentiment.py    # Sentiment analysis with LM dictionary
â”‚   â”‚   â”œâ”€â”€ dictionaries/   # Dictionary-based features
â”‚   â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”‚   â”œâ”€â”€ constants.py        # Feature category definitions
â”‚   â”‚   â”‚   â”œâ”€â”€ lm_dictionary.py    # Loughran-McDonald dictionary
â”‚   â”‚   â”‚   â””â”€â”€ schemas.py          # âš ï¸ Pydantic v2 data models (3 models)
â”‚   â”‚   â””â”€â”€ readability/    # âš ï¸ Text complexity & readability analysis (NEW)
â”‚   â”‚       â”œâ”€â”€ __init__.py
â”‚   â”‚       â”œâ”€â”€ constants.py        # Financial domain word list (200+ terms)
â”‚   â”‚       â”œâ”€â”€ schemas.py          # âš ï¸ Pydantic v2 models (3 models)
â”‚   â”‚       â””â”€â”€ analyzer.py         # ReadabilityAnalyzer (textstat + adjustments)
â”‚   â”‚
â”‚   â”œâ”€â”€ analysis/           # NLP analysis and inference
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ inference.py    # Model inference
â”‚   â”‚   â””â”€â”€ taxonomies/
â”‚   â”‚       â””â”€â”€ risk_taxonomy.yaml
â”‚   â”‚
â”‚   â””â”€â”€ visualization/      # Visualization and UI
â”‚       â”œâ”€â”€ __init__.py
â”‚       â””â”€â”€ app.py          # Streamlit dashboard
â”‚
â””â”€â”€ tests/                  # Unit and integration tests
    â”œâ”€â”€ test_preprocessing.py
    â””â”€â”€ test_analysis.py


# ========================================
# KEY CONCEPTS & GUIDANCE
# ========================================

## 1. PYDANTIC V2 ENFORCEMENT (MANDATORY)
#
# âš ï¸ ALL CODE MUST USE PYDANTIC V2.12.4+
#
# Before committing:
#   python scripts/utils/validation/validate_pydantic_v2.py src/
#
# See docs/PYDANTIC_V2_ENFORCEMENT.md for complete guide
# See docs/README.md for quick reference
#
# Quick Reference:
#   âŒ NEVER USE: @validator, class Config:, .dict(), .json()
#   âœ… ALWAYS USE: @field_validator, model_config =, .model_dump(), .model_dump_json()

## 2. CONFIGURATION MANAGEMENT
#
# The project uses a multi-layer configuration system:
#
# Layer 1: YAML Defaults
#   - configs/config.yaml (main settings)
#   - configs/features/sentiment.yaml (sentiment analysis)
#   - configs/features/readability.yaml (readability/complexity)
#
# Layer 2: Pydantic Settings Models (src/config.py)
#   - 21 Pydantic v2 BaseSettings models (incl. ReadabilityConfig)
#   - Type-safe with validation
#   - Environment variable support with prefixes
#
# Layer 3: Environment Variables (.env)
#   - Override YAML defaults at runtime
#   - Use env_prefix for namespacing (e.g., PATHS_*, SENTIMENT_*)
#
# Layer 4: Legacy Exports (backward compatibility)
#   - Module-level variables for easy imports
#
# See docs/ENUM_CONFIG_PATTERNS.md for patterns
# See docs/CONFIG_MIGRATION_GUIDE.md for migration

## 3. READABILITY ANALYSIS (TEXT COMPLEXITY AS RISK PROXY)
#
# The readability module measures text complexity in SEC filings.
# Research shows harder-to-read 10-Ks are associated with:
#   - Higher future stock volatility
#   - More negative earnings surprises
#   - Higher audit fees
#   - Potential obfuscation of bad news
#
# Key Components:
#
# 1. Standard Readability Indices (via textstat library):
#   - Flesch-Kincaid Grade Level (most widely used in research)
#   - Gunning Fog Index (better for business/technical writing)
#   - Flesch Reading Ease (0-100 scale)
#   - SMOG Index, ARI, Coleman-Liau Index
#
# 2. Financial Domain Adjustments:
#   - FINANCIAL_COMMON_WORDS: 200+ curated 3+ syllable terms
#   - Excludes common financial words from "complex word" count
#   - Examples excluded: investment, management, financial, regulatory
#   - Impact: Reduces false positives by 35-50% for 10-Ks
#
# 3. Custom Obfuscation Score (0-100):
#   - Weighted composite of multiple readability signals
#   - <40: Clear, 40-60: Typical 10-K, 60-75: Elevated, >75: High risk
#   - Combines FK grade, Fog index, sentence length, complex words
#
# 4. Structural Complexity Metrics:
#   - Average sentence length, word length, syllables per word
#   - Percentage of long sentences (>30 words)
#   - Paragraph length analysis
#
# Module Structure (follows dictionaries/ pattern):
#   - constants.py: Immutable (financial word list, benchmarks, citations)
#   - schemas.py: Pydantic v2 models (ReadabilityFeatures, Metadata, Result)
#   - analyzer.py: ReadabilityAnalyzer class (textstat + adjustments)
#
# Usage:
#   from src.features import ReadabilityAnalyzer
#   from src.preprocessing.cleaning import TextCleaner
#
#   cleaner = TextCleaner(remove_punctuation=False)  # Keep for sentences!
#   cleaned = cleaner.clean_html_text(raw_text)
#
#   analyzer = ReadabilityAnalyzer()
#   features = analyzer.extract_features(cleaned)
#   print(f"Gunning Fog: {features.gunning_fog_index}")
#   print(f"Obfuscation Score: {features.obfuscation_score}/100")
#
# Academic References:
#   - Li (2008): Annual report readability and earnings persistence
#   - Loughran & McDonald (2014): Measuring readability in financial disclosures

## 4. PROJECT STRUCTURE PHILOSOPHY
#
# Separation of Concerns:
#   src/          - Production code (clean, tested, importable)
#   scripts/      - Pipeline execution (numbered by ML stage)
#   examples/     - Demonstration and learning
#   tests/        - Quality assurance
#
# Data Organization:
#   raw/          - Original, immutable data
#   interim/      - Intermediate transformations
#   processed/    - Final, model-ready datasets
#
# Scripts Organization (ML Lifecycle):
#   01-10 prefix  - Natural execution order
#   utils/        - Cross-cutting tools (debugging, validation, testing)

## 5. DEVELOPMENT WORKFLOW
#
# 1. Setup:
#    git clone https://github.com/bethCoderNewbie/SEC-finetune.git
#    cd SEC-finetune
#    pip install -e ".[dev]"
#    python scripts/utils/setup_nlp_models.py
#
# 2. Before Coding:
#    - Review docs/PYDANTIC_V2_ENFORCEMENT.md
#    - Check existing patterns in src/config.py and src/features/dictionaries/schemas.py
#
# 3. During Development:
#    - Use Pydantic v2 patterns only
#    - Add type hints to all functions
#    - Document with docstrings
#
# 4. Before Committing:
#    python scripts/utils/validation/validate_pydantic_v2.py <your_files>
#    ruff check src/ scripts/
#    mypy src/
#
# 5. Commit:
#    git add <files>
#    git commit -m "Description"
#    git push

## 6. IMPORTANT FILES
#
# Configuration:
#   - src/config.py              (21 Pydantic Settings models)
#   - configs/config.yaml        (YAML defaults)
#   - configs/features/sentiment.yaml
#   - configs/features/readability.yaml
#
# Data Models:
#   - src/features/dictionaries/schemas.py  (3 Pydantic data models)
#   - src/features/readability/schemas.py   (3 Pydantic data models)
#
# Validation:
#   - scripts/utils/validation/validate_pydantic_v2.py
#
# Documentation:
#   - docs/PYDANTIC_V2_ENFORCEMENT.md  (enforcement guide)
#   - docs/README.md                    (documentation index)
#   - docs/ENUM_CONFIG_PATTERNS.md     (configuration patterns)

## 7. DEPENDENCY MANAGEMENT
#
# âœ… USE: pyproject.toml (modern, PEP 621 compliant)
# âŒ NEVER: requirements.txt, requirements-dev.txt (removed)
#
# Install:
#   pip install -e .              # Base dependencies
#   pip install -e ".[dev]"       # With dev tools (ruff, mypy, pytest)
#   pip install -e ".[all]"       # Everything
#
# Key Dependencies:
#   - pydantic>=2.12.4           # ENFORCED
#   - pydantic-settings>=2.0.0   # For configuration
#   - sec-parser==0.54.0         # SEC filing parsing
#   - transformers>=4.35.0       # NLP models
#   - streamlit>=1.28.0          # Visualization
#   - textstat>=0.7.0            # Readability metrics

## 8. ML PIPELINE STAGES
#
# 01: Data Collection    â†’ Download SEC filings from EDGAR
# 02: Preprocessing      â†’ Parse HTML, extract sections
# 03: EDA               â†’ Exploratory data analysis
# 04: Feature Engineering â†’ Extract sentiment, readability, embeddings
# 05: Data Splitting    â†’ Create train/val/test splits
# 06: Training          â†’ Train/fine-tune models
# 07: Evaluation        â†’ Calculate metrics
# 08: Inference         â†’ Run predictions
# 09: Deployment        â†’ Deploy to production (planned)
# 10: Monitoring        â†’ Monitor performance (planned)

## 9. CODE QUALITY STANDARDS
#
# Enforced:
#   - Pydantic v2.12.4+ (automated validation)
#   - Type hints (mypy checking)
#   - Code formatting (black, ruff)
#   - Docstrings (all public functions)
#
# Pre-commit Checks:
#   - Pydantic v2 validation
#   - Ruff linting
#   - MyPy type checking
#   - Pytest (when available)

## 10. CURRENT PROJECT STATUS
#
# âœ… Complete:
#   - Pydantic v2 enforcement system
#   - Configuration refactor to YAML + Pydantic Settings
#   - Sentiment analysis with Loughran-McDonald dictionary
#   - Readability/complexity analysis with financial domain adjustments
#   - Text cleaning pipeline (TextCleaner with HTML/boilerplate removal)
#   - Feature extraction pipeline (sentiment + readability integrated)
#   - Validation automation
#   - Comprehensive documentation
#
# ğŸš§ In Progress:
#   - Model training infrastructure
#   - Evaluation metrics
#
# ğŸ“‹ Planned:
#   - Deployment automation
#   - Monitoring dashboards
#   - CI/CD pipelines

## 11. GETTING HELP
#
# Documentation:
#   - docs/README.md                    - Start here
#   - docs/PYDANTIC_V2_ENFORCEMENT.md  - Pydantic v2 guide
#   - scripts/utils/README.md          - Utility scripts
#   - scripts/README.md                - ML pipeline scripts
#
# Validation:
#   python scripts/utils/validation/validate_pydantic_v2.py --help
#
# Examples:
#   - examples/01_basic_extraction.py
#   - examples/03_sentiment_analysis.py
#
# Debugging:
#   - scripts/utils/debugging/diagnose_extraction.py
#   - scripts/utils/inspection/inspect_parsed.py

# ========================================
# END OF REPOSITORY STRUCTURE
# ========================================
