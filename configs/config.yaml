# ============================================
# SEC Filing Analyzer - Configuration
# ============================================
# This file contains all default configuration values.
# Environment variables can override any setting.

# ===========================
# SEC Parser Configuration
# ===========================
sec_parser:
  # Supported form types
  supported_form_types:
    - "10-K"
    - "10-Q"

  # Default form type
  default_form_type: "10-K"

  # Input file types
  # Options: ["html"], ["txt"], or ["html", "txt"] for both
  # Note: sec-parser requires HTML format for semantic parsing
  input_file_extensions:
    - "html"

  # Parsing configuration
  parse_tables: true
  parse_images: false

  # Version
  version: "0.54.0"

# ===========================
# Model Configuration
# ===========================
models:
  # Model for financial text analysis
  default_model: "ProsusAI/finbert"

  # Zero-shot classification model
  zero_shot_model: "facebook/bart-large-mnli"

# ===========================
# Preprocessing Configuration
# ===========================
preprocessing:
  # Segmentation settings
  min_segment_length: 50
  max_segment_length: 999999999999
  min_segment_words: 20

  # Text cleaning settings
  remove_html_tags: true
  normalize_whitespace: true
  remove_page_numbers: true

  # HTML Sanitization (pre-parser cleaning)
  # These settings improve ParsedFiling quality by cleaning HTML before sec-parser
  sanitizer:
    # Enable pre-parser HTML sanitization
    enabled: true
    # Remove SEC EDGAR submission header boilerplate
    # WARNING: Set to false to preserve metadata (CIK, SIC, company name)
    remove_edgar_header: false
    # Remove EDGAR SGML tags (<PAGE>, <S>, <C>, etc.)
    # WARNING: Set to false - these tags are used by sec-parser for structure
    remove_edgar_tags: false
    # Decode HTML entities (&amp; â†’ &)
    decode_entities: true
    # Apply NFKC Unicode normalization
    normalize_unicode: true
    # Remove zero-width spaces and control characters
    remove_invisible_chars: true
    # Convert curly/smart quotes to straight ASCII quotes
    normalize_quotes: true
    # Attempt to fix mojibake (requires ftfy library)
    fix_encoding: false
    # Remove redundant nested tags (div, span, font)
    flatten_nesting: true

  # Parallel Processing & Timeout Configuration
  # Task timeout in seconds (20 minutes = 1200 seconds)
  # Offline batch processing can tolerate latency for data quality
  task_timeout: 1200

  # Resource limits
  max_workers: null  # Auto-detect based on CPU count
  max_tasks_per_child: 50  # Restart workers for memory management

  # File size monitoring (for logging only, not rejection)
  # Large files are processed to maintain dataset integrity (no bias)
  warn_file_size_mb: 50

  # Recursion limits (dynamic scaling for deeply nested HTML)
  recursion_limit_base: 10000
  recursion_limit_per_mb: 2000
  recursion_limit_max: 100000  # Increased from default 50,000

# ===========================
# Extraction Configuration
# ===========================
extraction:
  # Minimum confidence for extraction acceptance
  min_confidence: 0.7

  # Enable audit logging
  enable_audit_logging: true

  # Output format for extracted sections
  # Options: "json", "parquet", "both"
  output_format: "json"

  # Default sections to extract (can be overridden per job)
  default_sections:
    - "part1item1a"  # Risk Factors

# ===========================
# SEC Section Identifiers
# ===========================
sec_sections:
  # 10-K sections
  "10-K":
    part1item1: "Item 1. Business"
    part1item1a: "Item 1A. Risk Factors"
    part1item1b: "Item 1B. Unresolved Staff Comments"
    part1item1c: "Item 1C. Cybersecurity"
    part2item7: "Item 7. Management's Discussion and Analysis"
    part2item7a: "Item 7A. Quantitative and Qualitative Disclosures About Market Risk"
    part2item8: "Item 8. Financial Statements and Supplementary Data"

  # 10-Q sections
  "10-Q":
    part1item1: "Item 1. Financial Statements"
    part1item2: "Item 2. Management's Discussion and Analysis"
    part2item1a: "Item 1A. Risk Factors"

# ===========================
# Testing & Validation
# ===========================
testing:
  # Enable golden dataset validation
  enable_golden_validation: false

# ===========================
# Reproducibility
# ===========================
reproducibility:
  # Random seed for reproducibility
  random_seed: 42

# ===========================
# Naming Convention
# ===========================
naming:
  # Timestamp format for run_id (strftime format)
  timestamp_format: "%Y%m%d_%H%M%S"

  # Folder naming patterns
  # Placeholders: {run_id}, {name}, {git_sha}
  folder_pattern: "{run_id}_{name}_{git_sha}"
  folder_pattern_no_sha: "{run_id}_{name}"

  # File naming pattern
  # Placeholders: {stem}, {run_id}, {output_type}
  file_pattern: "{stem}_{run_id}_{output_type}.json"

  # Output type suffixes for each processing stage
  output_types:
    parsed: "parsed"
    extracted: "extracted_risks"
    cleaned: "cleaned_risks"
    segmented: "segmented_risks"
    labeled: "labeled"
