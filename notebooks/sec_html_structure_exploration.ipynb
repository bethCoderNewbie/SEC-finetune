{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SEC 10-K HTML File Structure: An Exploratory Walkthrough\n",
    "\n",
    "**Purpose:** Hands-on exploration of the raw EDGAR full-submission text files in `data/raw/`.\n",
    "Each `.html` file is **not** a standard HTML file — it is an SGML container embedding\n",
    "88–262 separate documents (HTML, XML, JSON, images, ZIP) concatenated inside a single file.\n",
    "\n",
    "**Reference files (from research doc `2026-02-22_12-00-00_sec_html_structure_and_extraction.md`):**\n",
    "- `AAPL_10K_2021.html` — 10.0 MB, 88 embedded docs\n",
    "- `ADI_10K_2025.html` — 13.1 MB, 117 embedded docs\n",
    "- `ALL_10K_2025.html` — 44.3 MB, 262 embedded docs\n",
    "\n",
    "---\n",
    "**Sections:**\n",
    "1. Setup & Load\n",
    "2. Layer 1 — SGML Container & Document Index\n",
    "3. Layer 2a — SGML Header Metadata\n",
    "4. Layer 2b — DEI iXBRL Tags\n",
    "5. Layer 3 — Document Index Distributions (cross-file)\n",
    "6. Layer 4 — High-Value Sub-Documents (MetaLinks, FilingSummary, R*.htm)\n",
    "7. Layer 5 — Universal Corpus Patterns\n",
    "8. Extraction Coverage: What We Get vs. What's Available"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup & Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "import sys\n",
    "from collections import Counter, defaultdict\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as mticker\n",
    "from xml.etree import ElementTree as ET\n",
    "\n",
    "# Add repo root to path so we can import project modules if needed\n",
    "REPO_ROOT = Path(\"../\").resolve()\n",
    "sys.path.insert(0, str(REPO_ROOT))\n",
    "\n",
    "RAW_DATA_DIR = REPO_ROOT / \"data\" / \"raw\"\n",
    "\n",
    "# Reference files from research document\n",
    "REF_FILES = {\n",
    "    \"AAPL_2021\": RAW_DATA_DIR / \"AAPL_10K_2021.html\",\n",
    "    \"ADI_2025\":  RAW_DATA_DIR / \"ADI_10K_2025.html\",\n",
    "    \"ALL_2025\":  RAW_DATA_DIR / \"ALL_10K_2025.html\",\n",
    "}\n",
    "\n",
    "# Verify files exist\n",
    "for name, path in REF_FILES.items():\n",
    "    size_mb = path.stat().st_size / 1e6 if path.exists() else None\n",
    "    status = f\"{size_mb:.1f} MB\" if size_mb else \"MISSING\"\n",
    "    print(f\"  {name:<12} {path.name:<25} {status}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the smallest reference file into memory for interactive exploration\n",
    "aapl_path = REF_FILES[\"AAPL_2021\"]\n",
    "print(f\"Loading {aapl_path.name} ...\")\n",
    "aapl_text = aapl_path.read_text(encoding=\"utf-8\", errors=\"replace\")\n",
    "print(f\"  Loaded: {len(aapl_text):,} characters  ({len(aapl_text)/1e6:.1f} MB)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Layer 1 — SGML Container & Document Index\n",
    "\n",
    "Every file begins with an `<SEC-HEADER>` block followed by a flat sequence of\n",
    "`<DOCUMENT>` entries, each with a `<TYPE>`, `<SEQUENCE>`, `<FILENAME>`, optional\n",
    "`<DESCRIPTION>`, and `<TEXT>` content block."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the first 100 lines of the raw file to see the SGML structure\n",
    "lines = aapl_text.splitlines()\n",
    "print(f\"Total lines: {len(lines):,}\")\n",
    "print(\"\\n--- First 80 lines (SGML header + first document boundary) ---\")\n",
    "for i, line in enumerate(lines[:80], 1):\n",
    "    print(f\"{i:>4}  {line}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------------------------------\n",
    "# build_doc_index: single-pass parse of SGML <DOCUMENT> boundaries\n",
    "# -----------------------------------------------------------------------\n",
    "\n",
    "_DOC_BOUNDARY = re.compile(\n",
    "    r\"<DOCUMENT>\\s*\"\n",
    "    r\"<TYPE>([^\\n]+)\\n\"\n",
    "    r\"<SEQUENCE>([^\\n]+)\\n\"\n",
    "    r\"<FILENAME>([^\\n]+)\\n\"\n",
    "    r\"(?:<DESCRIPTION>([^\\n]*)\\n)?\"  # optional\n",
    "    r\"<TEXT>\",\n",
    "    re.IGNORECASE,\n",
    ")\n",
    "_TEXT_END = re.compile(r\"</TEXT>\", re.IGNORECASE)\n",
    "_INNER_WRAPPER = re.compile(r\"^<(?:XBRL|XML|JSON)>\\s*\", re.IGNORECASE)\n",
    "_INNER_WRAPPER_CLOSE = re.compile(r\"\\s*</(?:XBRL|XML|JSON)>$\", re.IGNORECASE)\n",
    "\n",
    "\n",
    "def build_doc_index(text: str) -> list[dict]:\n",
    "    \"\"\"Parse SGML <DOCUMENT> boundaries. Single pass; O(1) subsequent access.\"\"\"\n",
    "    index = []\n",
    "    for m in _DOC_BOUNDARY.finditer(text):\n",
    "        text_start = m.end()\n",
    "        end_m = _TEXT_END.search(text, text_start)\n",
    "        text_end = end_m.start() if end_m else len(text)\n",
    "        index.append({\n",
    "            \"seq\":        m.group(2).strip(),\n",
    "            \"type\":       m.group(1).strip(),\n",
    "            \"filename\":   m.group(3).strip(),\n",
    "            \"desc\":       (m.group(4) or \"\").strip(),\n",
    "            \"text_start\": text_start,\n",
    "            \"text_end\":   text_end,\n",
    "            \"size_bytes\": text_end - text_start,\n",
    "        })\n",
    "    return index\n",
    "\n",
    "\n",
    "def get_doc(text: str, index: list[dict], filename: str) -> tuple[str | None, dict | None]:\n",
    "    \"\"\"Extract a named document from the pre-built index, stripping inner wrappers.\"\"\"\n",
    "    entry = next((d for d in index if d[\"filename\"].lower() == filename.lower()), None)\n",
    "    if not entry:\n",
    "        return None, None\n",
    "    raw = text[entry[\"text_start\"]:entry[\"text_end\"]].strip()\n",
    "    raw = _INNER_WRAPPER.sub(\"\", raw)\n",
    "    raw = _INNER_WRAPPER_CLOSE.sub(\"\", raw)\n",
    "    return raw.strip(), entry\n",
    "\n",
    "\n",
    "# Build the AAPL index\n",
    "aapl_index = build_doc_index(aapl_text)\n",
    "print(f\"AAPL 2021 — {len(aapl_index)} embedded documents\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the full document index as a DataFrame\n",
    "df_index = pd.DataFrame([\n",
    "    {\n",
    "        \"seq\": d[\"seq\"],\n",
    "        \"type\": d[\"type\"],\n",
    "        \"filename\": d[\"filename\"],\n",
    "        \"desc\": d[\"desc\"][:60],\n",
    "        \"size_KB\": round(d[\"size_bytes\"] / 1024, 1),\n",
    "    }\n",
    "    for d in aapl_index\n",
    "])\n",
    "print(f\"Shape: {df_index.shape}\")\n",
    "df_index.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Document type distribution\n",
    "type_counts = Counter(d[\"type\"] for d in aapl_index)\n",
    "type_sizes = defaultdict(int)\n",
    "for d in aapl_index:\n",
    "    type_sizes[d[\"type\"]] += d[\"size_bytes\"]\n",
    "\n",
    "df_types = pd.DataFrame([\n",
    "    {\"type\": t, \"count\": c, \"total_size_KB\": round(type_sizes[t] / 1024, 1)}\n",
    "    for t, c in type_counts.most_common()\n",
    "])\n",
    "print(\"AAPL 2021 — Document type distribution:\")\n",
    "df_types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the document boundary pattern in raw text for the first two documents\n",
    "# (find where \"<DOCUMENT>\" first appears and show 30 lines around each)\n",
    "doc_starts = [m.start() for m in re.finditer(r\"<DOCUMENT>\", aapl_text, re.IGNORECASE)]\n",
    "print(f\"Number of <DOCUMENT> tags: {len(doc_starts)}\")\n",
    "print(f\"\\n--- First document boundary (chars {doc_starts[0]}–{doc_starts[0]+400}) ---\")\n",
    "print(aapl_text[doc_starts[0]:doc_starts[0]+400])\n",
    "print(f\"\\n--- Second document boundary (chars {doc_starts[1]}–{doc_starts[1]+200}) ---\")\n",
    "print(aapl_text[doc_starts[1]:doc_starts[1]+200])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Layer 2a — SGML Header Metadata\n",
    "\n",
    "The `<SEC-HEADER>` block (lines 1–~80) contains plain-text key-value pairs\n",
    "for company identity, filing dates, and SIC classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the raw SGML header\n",
    "header_m = re.search(r\"<SEC-HEADER>(.*?)</SEC-HEADER>\", aapl_text, re.DOTALL | re.IGNORECASE)\n",
    "if header_m:\n",
    "    print(\"--- Raw SEC-HEADER block ---\")\n",
    "    print(header_m.group(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SGML header field parser\n",
    "_SIC_CODE = re.compile(r\"\\[(\\d+)\\]\")\n",
    "\n",
    "_HEADER_FIELDS = [\n",
    "    (r\"CONFORMED SUBMISSION TYPE:\\s*(.+)\",   \"submission_type\"),\n",
    "    (r\"PUBLIC DOCUMENT COUNT:\\s*(.+)\",        \"document_count\"),\n",
    "    (r\"CONFORMED PERIOD OF REPORT:\\s*(.+)\",   \"period_of_report\"),\n",
    "    (r\"FILED AS OF DATE:\\s*(.+)\",             \"filed_as_of_date\"),\n",
    "    (r\"ACCESSION NUMBER:\\s*(.+)\",             \"accession_number\"),\n",
    "    (r\"COMPANY CONFORMED NAME:\\s*(.+)\",       \"company_name\"),\n",
    "    (r\"CENTRAL INDEX KEY:\\s*(.+)\",            \"cik\"),\n",
    "    (r\"STANDARD INDUSTRIAL CLASSIFICATION:\\s*(.+)\", \"sic_full\"),\n",
    "    (r\"EIN:\\s*(.+)\",                          \"ein\"),\n",
    "    (r\"STATE OF INCORPORATION:\\s*(.+)\",       \"state_of_incorporation\"),\n",
    "    (r\"FISCAL YEAR END:\\s*(.+)\",              \"fiscal_year_end\"),\n",
    "    (r\"SEC FILE NUMBER:\\s*(.+)\",              \"sec_file_number\"),\n",
    "    (r\"FORM TYPE:\\s*(.+)\",                    \"form_type\"),\n",
    "]\n",
    "\n",
    "def parse_sgml_header(text: str) -> dict:\n",
    "    hm = re.search(r\"<SEC-HEADER>(.*?)</SEC-HEADER>\", text, re.DOTALL | re.IGNORECASE)\n",
    "    if not hm:\n",
    "        return {}\n",
    "    header = hm.group(1)\n",
    "    result = {}\n",
    "    for pattern, key in _HEADER_FIELDS:\n",
    "        m = re.search(pattern, header, re.IGNORECASE)\n",
    "        if m:\n",
    "            result[key] = m.group(1).strip()\n",
    "    if \"sic_full\" in result:\n",
    "        sic_m = _SIC_CODE.search(result[\"sic_full\"])\n",
    "        if sic_m:\n",
    "            result[\"sic_code\"] = sic_m.group(1)\n",
    "            result[\"sic_name\"] = result[\"sic_full\"][:sic_m.start()].strip()\n",
    "    if \"period_of_report\" in result and len(result[\"period_of_report\"]) == 8:\n",
    "        result[\"fiscal_year\"] = result[\"period_of_report\"][:4]\n",
    "    return result\n",
    "\n",
    "\n",
    "aapl_header = parse_sgml_header(aapl_text)\n",
    "pd.DataFrame(list(aapl_header.items()), columns=[\"field\", \"value\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Layer 2b — DEI iXBRL Tags\n",
    "\n",
    "The main 10-K document contains DEI (Document and Entity Information) iXBRL tags\n",
    "in its `<ix:hidden>` block. These provide machine-readable company identity data\n",
    "richer than the SGML header — including the **ticker symbol** (not in SGML header)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the main 10-K document\n",
    "main_entry = next((d for d in aapl_index if d[\"type\"].upper() == \"10-K\"), None)\n",
    "print(f\"Main 10-K document: {main_entry['filename']}  ({main_entry['size_bytes']/1e6:.2f} MB)\")\n",
    "main_html, _ = get_doc(aapl_text, aapl_index, main_entry[\"filename\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show a snippet of the ix:hidden block where DEI tags live\n",
    "hidden_m = re.search(r\"<ix:hidden>(.*?)</ix:hidden>\", main_html, re.DOTALL | re.IGNORECASE)\n",
    "if hidden_m:\n",
    "    hidden = hidden_m.group(1)\n",
    "    print(f\"<ix:hidden> block: {len(hidden):,} chars\")\n",
    "    # Show first 2000 chars\n",
    "    print(\"\\n--- First 2000 chars of ix:hidden ---\")\n",
    "    print(hidden[:2000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEI tag extractor\n",
    "_DEI_TAGS = [\n",
    "    \"dei:EntityCentralIndexKey\",\n",
    "    \"dei:TradingSymbol\",\n",
    "    \"dei:EntityRegistrantName\",\n",
    "    \"dei:DocumentFiscalYearFocus\",\n",
    "    \"dei:DocumentFiscalPeriodFocus\",\n",
    "    \"dei:DocumentType\",\n",
    "    \"dei:DocumentPeriodEndDate\",\n",
    "    \"dei:EntityIncorporationStateCountryCode\",\n",
    "    \"dei:EntityTaxIdentificationNumber\",\n",
    "    \"dei:EntityAddressAddressLine1\",\n",
    "    \"dei:EntityAddressCityOrTown\",\n",
    "    \"dei:EntityAddressStateOrProvince\",\n",
    "    \"dei:EntityAddressPostalZipCode\",\n",
    "    \"dei:CityAreaCode\",\n",
    "    \"dei:LocalPhoneNumber\",\n",
    "    \"dei:Security12bTitle\",\n",
    "    \"dei:SecurityExchangeName\",\n",
    "    \"dei:EntityWellKnownSeasonedIssuer\",\n",
    "    \"dei:EntityFilerCategory\",\n",
    "    \"dei:EntityPublicFloat\",\n",
    "    \"dei:EntityCommonStockSharesOutstanding\",\n",
    "    \"dei:AmendmentFlag\",\n",
    "    \"dei:IcfrAuditorAttestationFlag\",\n",
    "]\n",
    "\n",
    "_IX_FACT = re.compile(\n",
    "    r'<ix:(?:non(?:Numeric|Fraction)|numeric)\\b[^>]*\\bname=[\"\\']([^\"\\']+)[\"\\'][^>]*>'\n",
    "    r\"(.*?)\"\n",
    "    r\"</ix:(?:non(?:Numeric|Fraction)|numeric)>\",\n",
    "    re.DOTALL | re.IGNORECASE,\n",
    ")\n",
    "_HTML_TAG = re.compile(r\"<[^>]+>\")\n",
    "\n",
    "\n",
    "def extract_dei_tags(html: str) -> dict[str, str]:\n",
    "    dei_set = set(_DEI_TAGS)\n",
    "    result = {}\n",
    "    for m in _IX_FACT.finditer(html):\n",
    "        name = m.group(1).strip()\n",
    "        if name in dei_set and name not in result:\n",
    "            raw = _HTML_TAG.sub(\"\", m.group(2)).strip()\n",
    "            if raw:\n",
    "                result[name] = raw\n",
    "    return result\n",
    "\n",
    "\n",
    "aapl_dei = extract_dei_tags(main_html)\n",
    "print(f\"Found {len(aapl_dei)} / {len(_DEI_TAGS)} DEI tags\")\n",
    "pd.DataFrame([\n",
    "    {\"tag\": t, \"value\": aapl_dei.get(t, \"<not found>\")}\n",
    "    for t in _DEI_TAGS\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare SGML header vs DEI for the same fields\n",
    "comparison = [\n",
    "    {\"field\": \"Company Name\",         \"SGML Header\": aapl_header.get(\"company_name\"),          \"DEI iXBRL\": aapl_dei.get(\"dei:EntityRegistrantName\")},\n",
    "    {\"field\": \"CIK\",                  \"SGML Header\": aapl_header.get(\"cik\"),                   \"DEI iXBRL\": aapl_dei.get(\"dei:EntityCentralIndexKey\")},\n",
    "    {\"field\": \"EIN\",                  \"SGML Header\": aapl_header.get(\"ein\"),                   \"DEI iXBRL\": aapl_dei.get(\"dei:EntityTaxIdentificationNumber\")},\n",
    "    {\"field\": \"State\",                \"SGML Header\": aapl_header.get(\"state_of_incorporation\"), \"DEI iXBRL\": aapl_dei.get(\"dei:EntityIncorporationStateCountryCode\")},\n",
    "    {\"field\": \"Ticker\",               \"SGML Header\": \"<NOT IN SGML>\",                          \"DEI iXBRL\": aapl_dei.get(\"dei:TradingSymbol\")},\n",
    "    {\"field\": \"Exchange\",             \"SGML Header\": \"<NOT IN SGML>\",                          \"DEI iXBRL\": aapl_dei.get(\"dei:SecurityExchangeName\")},\n",
    "    {\"field\": \"Fiscal Year\",          \"SGML Header\": aapl_header.get(\"fiscal_year\"),            \"DEI iXBRL\": aapl_dei.get(\"dei:DocumentFiscalYearFocus\")},\n",
    "    {\"field\": \"Filer Category\",       \"SGML Header\": \"<NOT IN SGML>\",                          \"DEI iXBRL\": aapl_dei.get(\"dei:EntityFilerCategory\")},\n",
    "    {\"field\": \"Shares Outstanding\",   \"SGML Header\": \"<NOT IN SGML>\",                          \"DEI iXBRL\": aapl_dei.get(\"dei:EntityCommonStockSharesOutstanding\")},\n",
    "]\n",
    "pd.DataFrame(comparison)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Layer 3 — Document Index Distributions Across All Three Reference Files\n",
    "\n",
    "Compare the three reference files to see how document counts and types vary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build indexes for all three reference files\n",
    "print(\"Building document indexes for all three reference files...\")\n",
    "ref_data = {}\n",
    "for name, path in REF_FILES.items():\n",
    "    print(f\"  Loading {name} ({path.stat().st_size / 1e6:.1f} MB)...\")\n",
    "    text = path.read_text(encoding=\"utf-8\", errors=\"replace\")\n",
    "    index = build_doc_index(text)\n",
    "    header = parse_sgml_header(text)\n",
    "    ref_data[name] = {\"text\": text, \"index\": index, \"header\": header}\n",
    "    print(f\"    {len(index)} embedded documents\")\n",
    "\n",
    "print(\"\\nDone.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross-file document type distribution table\n",
    "all_types = sorted(set(\n",
    "    d[\"type\"]\n",
    "    for name in ref_data\n",
    "    for d in ref_data[name][\"index\"]\n",
    "))\n",
    "\n",
    "rows = []\n",
    "for doc_type in all_types:\n",
    "    row = {\"type\": doc_type}\n",
    "    for name in ref_data:\n",
    "        row[name] = sum(1 for d in ref_data[name][\"index\"] if d[\"type\"] == doc_type)\n",
    "    rows.append(row)\n",
    "\n",
    "df_cross = pd.DataFrame(rows).set_index(\"type\")\n",
    "df_cross[\"total\"] = df_cross.sum(axis=1)\n",
    "df_cross.sort_values(\"total\", ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bar chart: document counts by file and type (top 8 types)\n",
    "top_types = df_cross.nlargest(8, \"total\").drop(columns=\"total\")\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 5))\n",
    "top_types.T.plot(kind=\"bar\", ax=ax)\n",
    "ax.set_title(\"Embedded Document Count by Type — Three Reference Files\", fontsize=13)\n",
    "ax.set_xlabel(\"Filing\")\n",
    "ax.set_ylabel(\"Document Count\")\n",
    "ax.tick_params(axis=\"x\", rotation=0)\n",
    "ax.legend(title=\"Document Type\", bbox_to_anchor=(1.02, 1), loc=\"upper left\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Size breakdown by document type\n",
    "size_rows = []\n",
    "for name in ref_data:\n",
    "    type_sizes = defaultdict(int)\n",
    "    for d in ref_data[name][\"index\"]:\n",
    "        type_sizes[d[\"type\"]] += d[\"size_bytes\"]\n",
    "    for doc_type, sz in type_sizes.items():\n",
    "        size_rows.append({\"file\": name, \"type\": doc_type, \"size_MB\": round(sz / 1e6, 2)})\n",
    "\n",
    "df_sizes = pd.DataFrame(size_rows)\n",
    "\n",
    "# Pivot and plot\n",
    "pivot = df_sizes.pivot_table(index=\"type\", columns=\"file\", values=\"size_MB\", aggfunc=\"sum\").fillna(0)\n",
    "pivot[\"total\"] = pivot.sum(axis=1)\n",
    "top_size_types = pivot.nlargest(8, \"total\").drop(columns=\"total\")\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 5))\n",
    "top_size_types.plot(kind=\"bar\", ax=ax)\n",
    "ax.set_title(\"Content Size by Document Type (MB) — Three Reference Files\", fontsize=13)\n",
    "ax.set_xlabel(\"Document Type\")\n",
    "ax.set_ylabel(\"Total Size (MB)\")\n",
    "ax.tick_params(axis=\"x\", rotation=30)\n",
    "ax.legend(title=\"Filing\", bbox_to_anchor=(1.02, 1), loc=\"upper left\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Layer 4 — High-Value Sub-Documents\n",
    "\n",
    "Four documents contain most of the structured financial data. All are extractable\n",
    "by filename using the index built in Layer 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1 MetaLinks.json — XBRL Element Catalogue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract MetaLinks.json from AAPL 2021\n",
    "meta_raw, meta_entry = get_doc(aapl_text, aapl_index, \"MetaLinks.json\")\n",
    "if meta_raw:\n",
    "    meta = json.loads(meta_raw)\n",
    "    print(f\"MetaLinks.json size: {meta_entry['size_bytes'] / 1024:.0f} KB\")\n",
    "    print(f\"Top-level keys: {list(meta.keys())}\")\n",
    "    \n",
    "    # Instance-level statistics\n",
    "    instance_key = list(meta[\"instance\"].keys())[0]\n",
    "    inst = meta[\"instance\"][instance_key]\n",
    "    print(f\"\\nInstance: {instance_key}\")\n",
    "    stats_keys = [\"axisCustom\", \"axisStandard\", \"contextCount\", \"elementCount\",\n",
    "                  \"keyCustom\", \"keyStandard\", \"memberCustom\", \"memberStandard\",\n",
    "                  \"segmentCount\", \"unitCount\"]\n",
    "    for k in stats_keys:\n",
    "        print(f\"  {k:<22} {inst.get(k, 'N/A')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore the tag dictionary\n",
    "tags = inst.get(\"tag\", {})\n",
    "print(f\"Total XBRL elements (tags): {len(tags)}\")\n",
    "\n",
    "# Count by namespace prefix\n",
    "prefix_counts = Counter(k.split(\"_\")[0] for k in tags.keys())\n",
    "print(\"\\nNamespace prefix distribution:\")\n",
    "for prefix, count in prefix_counts.most_common():\n",
    "    print(f\"  {prefix:<20} {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show a sample element entry (the canonical MetaLinks structure)\n",
    "sample_key = next(k for k in tags if k.startswith(\"us-gaap_\"))\n",
    "print(f\"Sample element: {sample_key}\")\n",
    "print(json.dumps(tags[sample_key], indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# xbrltype distribution\n",
    "xbrl_types = Counter(v.get(\"xbrltype\", \"unknown\") for v in tags.values())\n",
    "print(\"xbrltype distribution:\")\n",
    "for t, c in xbrl_types.most_common():\n",
    "    print(f\"  {t:<30} {c}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Financial report index from MetaLinks (matches R*.htm filenames)\n",
    "reports = inst.get(\"report\", {})\n",
    "print(f\"Financial reports mapped: {len(reports)}\")\n",
    "report_rows = [\n",
    "    {\"key\": k, \"longName\": v.get(\"longName\", \"\"), \"menuCat\": v.get(\"menuCat\", \"\")}\n",
    "    for k, v in list(reports.items())[:20]\n",
    "]\n",
    "pd.DataFrame(report_rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 FilingSummary.xml — Report Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract FilingSummary.xml\n",
    "fs_raw, fs_entry = get_doc(aapl_text, aapl_index, \"FilingSummary.xml\")\n",
    "if fs_raw:\n",
    "    print(f\"FilingSummary.xml size: {fs_entry['size_bytes'] / 1024:.0f} KB\")\n",
    "    root = ET.fromstring(fs_raw)\n",
    "    \n",
    "    # Summary statistics from header elements\n",
    "    for tag in [\"Version\", \"ContextCount\", \"ElementCount\", \"SegmentCount\"]:\n",
    "        el = root.find(tag)\n",
    "        if el is not None:\n",
    "            print(f\"  {tag}: {el.text}\")\n",
    "    \n",
    "    # Extract report list\n",
    "    reports_el = root.find(\"MyReports\")\n",
    "    report_rows = []\n",
    "    for rpt in reports_el.findall(\"Report\"):\n",
    "        report_rows.append({\n",
    "            \"HtmlFileName\": (rpt.findtext(\"HtmlFileName\") or \"\").strip(),\n",
    "            \"LongName\": (rpt.findtext(\"LongName\") or \"\").strip(),\n",
    "            \"MenuCategory\": (rpt.findtext(\"MenuCategory\") or \"\").strip(),\n",
    "        })\n",
    "    df_reports = pd.DataFrame(report_rows)\n",
    "    print(f\"\\nTotal reports: {len(df_reports)}\")\n",
    "    df_reports.head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MenuCategory distribution\n",
    "cat_counts = df_reports[\"MenuCategory\"].value_counts()\n",
    "print(\"MenuCategory distribution:\")\n",
    "print(cat_counts.to_string())\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(7, 4))\n",
    "cat_counts.plot(kind=\"bar\", ax=ax, color=\"steelblue\")\n",
    "ax.set_title(\"FilingSummary.xml — Reports by MenuCategory (AAPL 2021)\", fontsize=12)\n",
    "ax.set_xlabel(\"MenuCategory\")\n",
    "ax.set_ylabel(\"Count\")\n",
    "ax.tick_params(axis=\"x\", rotation=30)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3 R*.htm — XBRL Financial Statement Sheets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract and show R2.htm (Consolidated Statements of Operations)\n",
    "r2_raw, r2_entry = get_doc(aapl_text, aapl_index, \"R2.htm\")\n",
    "if r2_raw:\n",
    "    print(f\"R2.htm size: {r2_entry['size_bytes'] / 1024:.0f} KB\")\n",
    "    print(\"\\n--- First 1500 chars of R2.htm ---\")\n",
    "    print(r2_raw[:1500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Size distribution of all R*.htm sheets\n",
    "r_sheets = [d for d in aapl_index if re.match(r\"R\\d+\\.htm\", d[\"filename\"], re.IGNORECASE)]\n",
    "r_sizes_kb = [d[\"size_bytes\"] / 1024 for d in r_sheets]\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "axes[0].hist(r_sizes_kb, bins=30, color=\"steelblue\", edgecolor=\"white\")\n",
    "axes[0].set_title(f\"R*.htm Sheet Sizes (AAPL 2021, n={len(r_sheets)})\", fontsize=12)\n",
    "axes[0].set_xlabel(\"Size (KB)\")\n",
    "axes[0].set_ylabel(\"Count\")\n",
    "\n",
    "axes[1].hist(r_sizes_kb, bins=30, color=\"steelblue\", edgecolor=\"white\", cumulative=True, density=True)\n",
    "axes[1].set_title(\"Cumulative Distribution of R*.htm Sizes\", fontsize=12)\n",
    "axes[1].set_xlabel(\"Size (KB)\")\n",
    "axes[1].set_ylabel(\"Cumulative Fraction\")\n",
    "axes[1].yaxis.set_major_formatter(mticker.PercentFormatter(xmax=1))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"R*.htm sheet count: {len(r_sheets)}\")\n",
    "print(f\"Sizes: min={min(r_sizes_kb):.1f} KB  max={max(r_sizes_kb):.1f} KB  mean={sum(r_sizes_kb)/len(r_sizes_kb):.1f} KB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.4 XBRL Instance Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "\n# Find the XBRL instance document (*_htm.xml)\nxbrl_entry = next(\n    (d for d in aapl_index if d[\"filename\"].endswith(\"_htm.xml\")),\n    None\n)\nprint(f\"XBRL instance: {xbrl_entry['filename']}  ({xbrl_entry['size_bytes']/1e6:.2f} MB)\")\nxbrl_raw, _ = get_doc(aapl_text, aapl_index, xbrl_entry[\"filename\"])\nxbrl_root = ET.fromstring(xbrl_raw)\n\nXBRLI = \"http://www.xbrl.org/2003/instance\"\n\n# -----------------------------------------------------------------------\n# Context period types — three types per XBRL 2003 schema:\n#   instant  : single point-in-time (balance sheet dates)\n#   duration : start/end date range (income statement periods)\n#   forever  : no time dimension (entity-level facts, e.g. CIK)\n# -----------------------------------------------------------------------\ncontext_types = Counter()\ncontexts_by_type = {\"instant\": [], \"duration\": [], \"forever\": []}\n\nfor ctx in xbrl_root.iter(f\"{{{XBRLI}}}context\"):\n    period = ctx.find(f\"{{{XBRLI}}}period\")\n    if period is None:\n        context_types[\"unknown\"] += 1\n    elif period.find(f\"{{{XBRLI}}}forever\") is not None:\n        context_types[\"forever\"] += 1\n        contexts_by_type[\"forever\"].append(ctx)\n    elif period.find(f\"{{{XBRLI}}}instant\") is not None:\n        context_types[\"instant\"] += 1\n        contexts_by_type[\"instant\"].append(ctx)\n    else:\n        context_types[\"duration\"] += 1\n        contexts_by_type[\"duration\"].append(ctx)\n\n# -----------------------------------------------------------------------\n# Unit types — two structures per XBRL 2003 schema:\n#   measure : simple unit  e.g. <measure>iso4217:USD</measure>\n#   divide  : ratio unit   e.g. USD/share for EPS facts\n#             <divide><unitNumerator>...</unitNumerator>\n#                    <unitDenominator>...</unitDenominator></divide>\n# -----------------------------------------------------------------------\nunit_types = Counter()\nunit_details = []\n\nfor unit in xbrl_root.iter(f\"{{{XBRLI}}}unit\"):\n    uid = unit.attrib.get(\"id\", \"\")\n    divide = unit.find(f\"{{{XBRLI}}}divide\")\n    if divide is not None:\n        unit_types[\"divide\"] += 1\n        num_el = divide.find(f\".//{{{XBRLI}}}unitNumerator/{{{XBRLI}}}measure\")\n        den_el = divide.find(f\".//{{{XBRLI}}}unitDenominator/{{{XBRLI}}}measure\")\n        num = num_el.text.strip() if num_el is not None else \"\"\n        den = den_el.text.strip() if den_el is not None else \"\"\n        unit_details.append({\"id\": uid, \"type\": \"divide\", \"value\": f\"{num} / {den}\"})\n    else:\n        measure = unit.find(f\"{{{XBRLI}}}measure\")\n        unit_types[\"measure\"] += 1\n        val = measure.text.strip() if measure is not None else \"\"\n        unit_details.append({\"id\": uid, \"type\": \"measure\", \"value\": val})\n\n# -----------------------------------------------------------------------\n# Facts — all non-structural top-level elements\n# precision vs decimals: mutually exclusive per schema; check both\n# decimals=\"INF\" means exact value (integer counts, flags)\n# decimals=\"-6\"  means accurate to nearest 10^6 (precision indicator,\n#                NOT a scale factor — raw value is always in base units)\n# -----------------------------------------------------------------------\ndef _local(tag):\n    return tag.split(\"}\")[-1] if \"}\" in tag else tag\n\nprecision_count = 0\ndecimals_count = 0\ndecimals_inf_count = 0\nfact_count = 0\n\nfor el in xbrl_root:\n    if _local(el.tag) in (\"context\", \"unit\", \"schemaRef\"):\n        continue\n    fact_count += 1\n    if \"precision\" in el.attrib:\n        precision_count += 1\n    if \"decimals\" in el.attrib:\n        decimals_count += 1\n        if el.attrib[\"decimals\"].upper() == \"INF\":\n            decimals_inf_count += 1\n\nprint(f\"\\nContext period types : {dict(context_types)}\")\nprint(f\"Unit types           : {dict(unit_types)}\")\nprint(f\"Total facts          : {fact_count}\")\nprint(f\"  decimals attr      : {decimals_count}  ({decimals_inf_count} INF)\")\nprint(f\"  precision attr     : {precision_count}\")\n"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "\n# Show one example of each context period type\nfor period_type in (\"instant\", \"duration\", \"forever\"):\n    examples = contexts_by_type[period_type]\n    if examples:\n        ctx = examples[0]\n        print(f\"--- {period_type.upper()} context (1 of {len(examples)}) ---\")\n        print(ET.tostring(ctx, encoding=\"unicode\"))\n        print()\n    else:\n        print(f\"--- {period_type.upper()} : none found in this filing ---\\n\")\n"
  },
  {
   "cell_type": "code",
   "id": "rpx81lndooh",
   "source": "\n# Show sample monetary us-gaap facts with corrected attribute handling\n#\n# decimals=\"-6\" means the raw value is accurate to the nearest 10^6.\n# It is a PRECISION INDICATOR, not a scale factor.\n# The XML value is always in base units (USD), never pre-scaled.\n# Example: 54763000000 with decimals=-6 → $54,763,000,000 ± $1,000,000\n#\n# precision vs decimals: mutually exclusive per XBRL 2003 schema.\n# Modern filings use decimals. Check both to be safe.\n\nprefix_re = re.compile(r\"\\{([^}]+)\\}(.+)\")\n\nus_gaap_facts = [\n    el for el in xbrl_root\n    if \"fasb.org/us-gaap\" in el.tag\n    and el.text and el.text.strip().lstrip(\"-\").isdigit()\n]\nprint(f\"Monetary us-gaap facts: {len(us_gaap_facts)}\")\nprint(f\"{'Element':<50} {'Value (USD)':>22}  {'unitRef':<15} {'precision_attr'}\")\nprint(\"-\" * 105)\nfor el in us_gaap_facts[:10]:\n    m = prefix_re.match(el.tag)\n    localname = m.group(2) if m else el.tag\n    val = int(el.text.strip())\n    unit_ref  = el.attrib.get(\"unitRef\", \"\")\n    # Check decimals first, fall back to precision (older filings)\n    prec_attr = el.attrib.get(\"decimals\") or el.attrib.get(\"precision\") or \"\"\n    attr_name = \"decimals\" if \"decimals\" in el.attrib else (\"precision\" if \"precision\" in el.attrib else \"—\")\n    print(f\"  {localname:<48} {val:>22,}  {unit_ref:<15} {attr_name}={prec_attr}\")\n",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show first 5 fact values (us-gaap monetary facts)\n",
    "us_gaap_facts = [\n",
    "    f for f in facts\n",
    "    if \"fasb.org/us-gaap\" in f.tag and f.text and f.text.strip().lstrip(\"-\").isdigit()\n",
    "]\n",
    "print(f\"Monetary us-gaap facts: {len(us_gaap_facts)}\")\n",
    "for f in us_gaap_facts[:8]:\n",
    "    m = prefix_re.match(f.tag)\n",
    "    localname = m.group(2) if m else f.tag\n",
    "    val = int(f.text.strip())\n",
    "    ctx_ref = f.attrib.get(\"contextRef\", \"\")\n",
    "    unit_ref = f.attrib.get(\"unitRef\", \"\")\n",
    "    decimals = f.attrib.get(\"decimals\", \"\")\n",
    "    # Values are in thousands (decimals=\"-6\" means reported in millions)\n",
    "    print(f\"  {localname:<50} {val:>20,}  [{unit_ref}, decimals={decimals}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Layer 5 — Universal Corpus Patterns\n",
    "\n",
    "Compare SGML header metadata across all three reference files to confirm universal patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross-file SGML header comparison\n",
    "header_rows = []\n",
    "for name, data in ref_data.items():\n",
    "    h = data[\"header\"]\n",
    "    idx = data[\"index\"]\n",
    "    header_rows.append({\n",
    "        \"file\": name,\n",
    "        \"company_name\": h.get(\"company_name\", \"\"),\n",
    "        \"sic_code\": h.get(\"sic_code\", \"\"),\n",
    "        \"sic_name\": h.get(\"sic_name\", \"\"),\n",
    "        \"fiscal_year\": h.get(\"fiscal_year\", \"\"),\n",
    "        \"period_of_report\": h.get(\"period_of_report\", \"\"),\n",
    "        \"filed_as_of_date\": h.get(\"filed_as_of_date\", \"\"),\n",
    "        \"state_of_incorporation\": h.get(\"state_of_incorporation\", \"\"),\n",
    "        \"fiscal_year_end\": h.get(\"fiscal_year_end\", \"\"),\n",
    "        \"document_count\": h.get(\"document_count\", \"\"),\n",
    "        \"r_sheet_count\": sum(1 for d in idx if re.match(r\"R\\d+\\.htm\", d[\"filename\"])),\n",
    "        \"file_size_MB\": round(ref_data[name][\"text\"].__sizeof__() / 1e6, 1),  # approx\n",
    "    })\n",
    "\n",
    "pd.DataFrame(header_rows).set_index(\"file\").T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEI tag coverage across all three reference files\n",
    "dei_coverage_rows = []\n",
    "for name, data in ref_data.items():\n",
    "    main_fn = next((d[\"filename\"] for d in data[\"index\"] if d[\"type\"].upper() == \"10-K\"), None)\n",
    "    if main_fn:\n",
    "        main_html_local, _ = get_doc(data[\"text\"], data[\"index\"], main_fn)\n",
    "        dei_local = extract_dei_tags(main_html_local) if main_html_local else {}\n",
    "        for tag in _DEI_TAGS:\n",
    "            dei_coverage_rows.append({\n",
    "                \"file\": name,\n",
    "                \"tag\": tag.split(\":\")[-1],\n",
    "                \"found\": tag in dei_local,\n",
    "                \"value\": dei_local.get(tag, \"\")[:50],\n",
    "            })\n",
    "\n",
    "df_dei_cov = pd.DataFrame(dei_coverage_rows)\n",
    "pivot_dei = df_dei_cov.pivot_table(index=\"tag\", columns=\"file\", values=\"found\", aggfunc=\"first\")\n",
    "print(\"DEI tag coverage (True = found):\")\n",
    "pivot_dei"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Extraction Coverage: What We Get vs. What's Available\n",
    "\n",
    "Summary of what the current pipeline extracts vs. what the raw files contain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coverage = [\n",
    "    # Data field, Available, Currently extracted, Source\n",
    "    (\"company_name\",          True,  True,  \"SGML header\"),\n",
    "    (\"cik\",                   True,  True,  \"SGML header\"),\n",
    "    (\"sic_code\",              True,  True,  \"SGML header\"),\n",
    "    (\"sic_name\",              True,  True,  \"SGML header\"),\n",
    "    (\"ticker\",                True,  True,  \"DEI iXBRL\"),\n",
    "    (\"fiscal_year\",           True,  True,  \"SGML header\"),\n",
    "    (\"period_of_report\",      True,  True,  \"SGML header\"),\n",
    "    (\"ein\",                   True,  False, \"SGML header\"),\n",
    "    (\"state_of_incorporation\",True,  False, \"SGML header\"),\n",
    "    (\"fiscal_year_end (MMDD)\",True,  False, \"SGML header\"),\n",
    "    (\"accession_number\",      True,  False, \"SGML header\"),\n",
    "    (\"sec_file_number\",       True,  False, \"SGML header\"),\n",
    "    (\"exchange (Nasdaq/NYSE)\",True,  False, \"DEI iXBRL\"),\n",
    "    (\"shares_outstanding\",    True,  False, \"DEI iXBRL\"),\n",
    "    (\"public_float\",          True,  False, \"DEI iXBRL\"),\n",
    "    (\"filer_category\",        True,  False, \"DEI iXBRL\"),\n",
    "    (\"amendment_flag\",        True,  False, \"DEI iXBRL\"),\n",
    "    (\"FASB element definitions\",True, False,\"MetaLinks.json\"),\n",
    "    (\"all financial facts\",   True,  False, \"XBRL instance XML\"),\n",
    "    (\"calculation tree\",      True,  False, \"EX-101.CAL / MetaLinks\"),\n",
    "    (\"named financial stmts\", True,  False, \"FilingSummary.xml\"),\n",
    "    (\"company logo / charts\", True,  False, \"GRAPHIC documents\"),\n",
    "]\n",
    "\n",
    "df_cov = pd.DataFrame(coverage, columns=[\"Data Field\", \"Available\", \"Extracted\", \"Source\"])\n",
    "df_cov[\"Status\"] = df_cov[\"Extracted\"].map({True: \"✓ Extracted\", False: \"✗ Not yet\"})\n",
    "\n",
    "# Style the output\n",
    "def highlight_status(row):\n",
    "    if row[\"Extracted\"]:\n",
    "        return [\"background-color: #d4edda\"] * len(row)\n",
    "    return [\"background-color: #f8d7da\"] * len(row)\n",
    "\n",
    "df_cov[[\"Data Field\", \"Status\", \"Source\"]].style.apply(\n",
    "    lambda row: highlight_status(df_cov.iloc[row.name]),\n",
    "    axis=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "k0n9ksjx1k",
   "source": "## 9. Distribution Analysis & Pattern Discovery\n\nLoad the JSON output from the most recent script run and explore distributions\nacross the full sample. Run the script with `--sample 50` or `--all` first to\nget statistically meaningful counts.\n\n```bash\npython scripts/eda/sec_html_structure_explorer.py --sample 50 --no-file-detail\n```\n",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "oegjmrgfcqj",
   "source": "\n# Load JSON results from last script run, build analysis DataFrame\nimport numpy as np\nfrom datetime import date as _date\n\nresults_path = REPO_ROOT / \"reports\" / \"sec_html_structure\" / \"results.json\"\nwith open(results_path) as f:\n    corpus_results = json.load(f)\nprint(f\"Loaded {len(corpus_results)} filing results from {results_path.name}\")\n\ndef _parse_yyyymmdd(s):\n    try:\n        return _date(int(s[:4]), int(s[4:6]), int(s[6:8]))\n    except Exception:\n        return None\n\nrows = []\nfor r in corpus_results:\n    h = r[\"sgml_header\"]\n    dei = r[\"dei_tags\"]\n    period  = _parse_yyyymmdd(h.get(\"period_of_report\", \"\"))\n    filed   = _parse_yyyymmdd(h.get(\"filed_as_of_date\", \"\"))\n    lag     = (filed - period).days if period and filed else None\n\n    # Parse public float — strip commas, handle \"B\" suffix (some filings store 41.91)\n    raw_float = dei.get(\"dei:EntityPublicFloat\", \"\").replace(\",\", \"\").replace(\"$\", \"\")\n    try:\n        public_float_b = float(raw_float) / 1e9\n    except ValueError:\n        public_float_b = None\n\n    rows.append({\n        \"stem\":              r[\"stem\"],\n        \"ticker\":            r[\"stem\"].split(\"_\")[0],\n        \"fiscal_year\":       int(h[\"fiscal_year\"]) if h.get(\"fiscal_year\", \"\").isdigit() else None,\n        \"sic_code\":          h.get(\"sic_code\", \"\"),\n        \"sic_name\":          h.get(\"sic_name\", \"\"),\n        \"state_of_inc\":      h.get(\"state_of_incorporation\", \"\"),\n        \"period_of_report\":  h.get(\"period_of_report\", \"\"),\n        \"filed_as_of_date\":  h.get(\"filed_as_of_date\", \"\"),\n        \"filing_lag_days\":   lag,\n        \"file_size_mb\":      r[\"file_size_bytes\"] / 1e6,\n        \"main_10k_mb\":       r[\"main_10k_size_bytes\"] / 1e6,\n        \"total_documents\":   r[\"total_documents\"],\n        \"r_sheet_count\":     r[\"r_sheet_count\"],\n        \"filer_category\":    dei.get(\"dei:EntityFilerCategory\", \"\").strip(),\n        \"exchange\":          dei.get(\"dei:SecurityExchangeName\", \"\").strip(),\n        \"wksi\":              dei.get(\"dei:EntityWellKnownSeasonedIssuer\", \"\").strip(),\n        \"inc_state\":         dei.get(\"dei:EntityIncorporationStateCountryCode\", \"\").strip(),\n        \"hq_state\":          dei.get(\"dei:EntityAddressStateOrProvince\", \"\").strip(),\n        \"public_float_b\":    public_float_b,\n        \"dei_tag_count\":     r[\"dei_tag_count\"],\n        # XBRL stats\n        \"xbrl_facts\":        r.get(\"xbrl\", {}).get(\"fact_count\"),\n        \"xbrl_contexts\":     r.get(\"xbrl\", {}).get(\"context_count\"),\n        \"xbrl_divide_units\": r.get(\"xbrl\", {}).get(\"unit_types\", {}).get(\"divide\", 0),\n    })\n\ndf = pd.DataFrame(rows)\nprint(f\"DataFrame: {df.shape[0]} rows × {df.shape[1]} columns\")\ndf.head(3)\n",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "rri8d5k244f",
   "source": "### 9.1 Filing Lag Distribution\n\nDays between fiscal period end and SEC filing date. Large accelerated filers must\nfile within **60 days**, accelerated within **75 days**, others within **90 days**.\n",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "62sdu6vi1vc",
   "source": "\nlag = df[\"filing_lag_days\"].dropna()\nprint(f\"Filing lag stats (n={len(lag)}):\")\nprint(lag.describe().round(1).to_string())\n\nfig, axes = plt.subplots(1, 2, figsize=(13, 4))\n\n# Histogram\naxes[0].hist(lag, bins=20, color=\"steelblue\", edgecolor=\"white\")\nfor x, label, color in [(60, \"60d LAF\", \"red\"), (75, \"75d AF\", \"orange\"), (90, \"90d other\", \"green\")]:\n    axes[0].axvline(x, color=color, linestyle=\"--\", linewidth=1.2, label=label)\naxes[0].set_title(\"Filing Lag Distribution\", fontsize=12)\naxes[0].set_xlabel(\"Days (period end → filed date)\")\naxes[0].set_ylabel(\"Count\")\naxes[0].legend(fontsize=9)\n\n# By filer category (boxplot if enough data, else strip)\ncats = df[df[\"filing_lag_days\"].notna()].copy()\ncat_order = cats.groupby(\"filer_category\")[\"filing_lag_days\"].median().sort_values().index.tolist()\ncat_data  = [cats[cats[\"filer_category\"] == c][\"filing_lag_days\"].values for c in cat_order]\naxes[1].boxplot(cat_data, labels=[c[:25] for c in cat_order], vert=True)\naxes[1].set_title(\"Filing Lag by Filer Category\", fontsize=12)\naxes[1].set_ylabel(\"Days\")\naxes[1].tick_params(axis=\"x\", rotation=20)\n\nplt.tight_layout()\nplt.show()\n\n# Top fastest / slowest\nprint(\"\\nFastest filers:\")\nprint(df[[\"stem\", \"filer_category\", \"filing_lag_days\"]].nsmallest(5, \"filing_lag_days\").to_string(index=False))\nprint(\"\\nSlowest filers:\")\nprint(df[[\"stem\", \"filer_category\", \"filing_lag_days\"]].nlargest(5, \"filing_lag_days\").to_string(index=False))\n",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "wmq5afzi8u",
   "source": "### 9.2 Filer Category, Exchange & WKSI Distribution\n",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "wemdwqmnk7i",
   "source": "\nfig, axes = plt.subplots(1, 3, figsize=(15, 4))\n\nfor ax, col, title in [\n    (axes[0], \"filer_category\", \"Filer Category\"),\n    (axes[1], \"exchange\",       \"Exchange Listed On\"),\n    (axes[2], \"wksi\",           \"Well-Known Seasoned Issuer\"),\n]:\n    counts = df[col].replace(\"\", \"Unknown\").value_counts()\n    counts.plot(kind=\"bar\", ax=ax, color=\"steelblue\", edgecolor=\"white\")\n    ax.set_title(title, fontsize=11)\n    ax.set_xlabel(\"\")\n    ax.set_ylabel(\"Count\")\n    ax.tick_params(axis=\"x\", rotation=30)\n    for bar in ax.patches:\n        ax.text(bar.get_x() + bar.get_width() / 2, bar.get_height() + 0.1,\n                str(int(bar.get_height())), ha=\"center\", va=\"bottom\", fontsize=9)\n\nplt.tight_layout()\nplt.show()\n\n# Summary tables\nfor col, label in [(\"filer_category\", \"Filer Category\"), (\"exchange\", \"Exchange\"), (\"wksi\", \"WKSI\")]:\n    print(f\"\\n{label}:\")\n    print(df[col].replace(\"\", \"—\").value_counts().to_string())\n",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "3gbvtxbgsl4",
   "source": "### 9.3 State of Incorporation & HQ State\n",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "z9k9lwzy2hc",
   "source": "\nfig, axes = plt.subplots(1, 2, figsize=(14, 4))\n\nfor ax, col, title in [\n    (axes[0], \"inc_state\", \"State of Incorporation (DEI)\"),\n    (axes[1], \"hq_state\",  \"HQ State (DEI address)\"),\n]:\n    counts = df[col].replace(\"\", \"Unknown\").value_counts().head(12)\n    counts.plot(kind=\"bar\", ax=ax, color=\"steelblue\", edgecolor=\"white\")\n    ax.set_title(title, fontsize=11)\n    ax.set_xlabel(\"\")\n    ax.set_ylabel(\"Count\")\n    ax.tick_params(axis=\"x\", rotation=35)\n\nplt.suptitle(\"Geographic Distribution\", fontsize=13, y=1.02)\nplt.tight_layout()\nplt.show()\n\nprint(\"State of incorporation (top 10):\")\nprint(df[\"inc_state\"].replace(\"\", \"—\").value_counts().head(10).to_string())\nprint(\"\\nHQ state (top 10):\")\nprint(df[\"hq_state\"].replace(\"\", \"—\").value_counts().head(10).to_string())\n",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "khrrsk5i639",
   "source": "### 9.4 File Size vs. Complexity Correlations\n\nHow strongly do file size, R*.htm sheet count, total documents, and XBRL fact count\nco-vary? Pearson r closer to ±1 = stronger linear relationship.\n",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "udthgfbu8o",
   "source": "\nmetrics = [\"file_size_mb\", \"main_10k_mb\", \"r_sheet_count\", \"total_documents\", \"xbrl_facts\", \"xbrl_contexts\"]\ncorr = df[metrics].corr(method=\"pearson\")\n\nfig, axes = plt.subplots(1, 2, figsize=(15, 5))\n\n# Correlation heatmap\nim = axes[0].imshow(corr, vmin=-1, vmax=1, cmap=\"RdBu_r\", aspect=\"auto\")\naxes[0].set_xticks(range(len(metrics)))\naxes[0].set_yticks(range(len(metrics)))\naxes[0].set_xticklabels(metrics, rotation=35, ha=\"right\", fontsize=9)\naxes[0].set_yticklabels(metrics, fontsize=9)\nfor i in range(len(metrics)):\n    for j in range(len(metrics)):\n        axes[0].text(j, i, f\"{corr.iloc[i, j]:.2f}\", ha=\"center\", va=\"center\",\n                     fontsize=8, color=\"white\" if abs(corr.iloc[i, j]) > 0.6 else \"black\")\nplt.colorbar(im, ax=axes[0], fraction=0.046)\naxes[0].set_title(\"Pearson Correlation Heatmap\", fontsize=12)\n\n# Scatter: file_size_mb vs r_sheet_count (strongest expected pair)\nvalid = df[[\"file_size_mb\", \"r_sheet_count\", \"sic_name\"]].dropna()\naxes[1].scatter(valid[\"file_size_mb\"], valid[\"r_sheet_count\"],\n                alpha=0.7, s=60, color=\"steelblue\", edgecolors=\"white\", linewidths=0.5)\nfor _, row in valid.iterrows():\n    axes[1].annotate(row[\"sic_name\"][:15], (row[\"file_size_mb\"], row[\"r_sheet_count\"]),\n                     fontsize=7, alpha=0.6, xytext=(3, 3), textcoords=\"offset points\")\nr_val = valid[\"file_size_mb\"].corr(valid[\"r_sheet_count\"])\naxes[1].set_title(f\"File Size vs R*.htm Count  (r={r_val:.2f})\", fontsize=12)\naxes[1].set_xlabel(\"Total File Size (MB)\")\naxes[1].set_ylabel(\"R*.htm Sheet Count\")\n\nplt.tight_layout()\nplt.show()\n\nprint(\"Correlation matrix:\")\nprint(corr.round(2).to_string())\n",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "hel76adrwn",
   "source": "### 9.5 Industry Complexity: SIC Code vs Filing Size\n",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "hfkhxoanelk",
   "source": "\n# Group by SIC name, show industries with at least 2 filings in sample\nsic_stats = (\n    df.groupby(\"sic_name\")\n    .agg(\n        n=(\"stem\", \"count\"),\n        avg_size_mb=(\"file_size_mb\", \"mean\"),\n        avg_r_sheets=(\"r_sheet_count\", \"mean\"),\n        avg_facts=(\"xbrl_facts\", \"mean\"),\n    )\n    .query(\"n >= 2\")\n    .sort_values(\"avg_size_mb\", ascending=False)\n)\n\nprint(\"Industries with ≥2 filings — sorted by avg file size:\")\nprint(sic_stats.round(1).to_string())\n\nif len(sic_stats) >= 2:\n    fig, axes = plt.subplots(1, 2, figsize=(14, 4))\n    labels = [s[:30] for s in sic_stats.index]\n    x = range(len(sic_stats))\n\n    axes[0].bar(x, sic_stats[\"avg_size_mb\"], color=\"steelblue\", edgecolor=\"white\")\n    axes[0].set_xticks(x); axes[0].set_xticklabels(labels, rotation=35, ha=\"right\", fontsize=8)\n    axes[0].set_title(\"Avg File Size (MB) by Industry\", fontsize=11)\n    axes[0].set_ylabel(\"MB\")\n\n    axes[1].bar(x, sic_stats[\"avg_r_sheets\"], color=\"teal\", edgecolor=\"white\")\n    axes[1].set_xticks(x); axes[1].set_xticklabels(labels, rotation=35, ha=\"right\", fontsize=8)\n    axes[1].set_title(\"Avg R*.htm Sheet Count by Industry\", fontsize=11)\n    axes[1].set_ylabel(\"Sheets\")\n\n    plt.tight_layout()\n    plt.show()\nelse:\n    print(\"\\nNeed more filings per SIC for chart — run --sample 50 or --all.\")\n",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "9fjhrjy1da",
   "source": "### 9.6 Fiscal Year Trend: Does Filing Complexity Grow Over Time?\n",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "al4gz1du94h",
   "source": "\nyear_stats = (\n    df.dropna(subset=[\"fiscal_year\"])\n    .groupby(\"fiscal_year\")\n    .agg(\n        n=(\"stem\", \"count\"),\n        avg_size_mb=(\"file_size_mb\", \"mean\"),\n        avg_r_sheets=(\"r_sheet_count\", \"mean\"),\n        avg_docs=(\"total_documents\", \"mean\"),\n        avg_facts=(\"xbrl_facts\", \"mean\"),\n    )\n    .sort_index()\n)\nprint(\"Complexity by fiscal year:\")\nprint(year_stats.round(1).to_string())\n\nif len(year_stats) >= 3:\n    fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n    yrs = year_stats.index\n\n    for ax, col, label, color in [\n        (axes[0], \"avg_r_sheets\",  \"Avg R*.htm Sheets\",  \"steelblue\"),\n        (axes[1], \"avg_size_mb\",   \"Avg File Size (MB)\", \"teal\"),\n        (axes[2], \"avg_facts\",     \"Avg XBRL Facts\",     \"coral\"),\n    ]:\n        ax.plot(yrs, year_stats[col], marker=\"o\", color=color, linewidth=2)\n        ax.set_title(label, fontsize=11)\n        ax.set_xlabel(\"Fiscal Year\")\n        ax.set_xticks(yrs)\n        ax.tick_params(axis=\"x\", rotation=30)\n\n    plt.suptitle(\"Filing Complexity Trend Over Time\", fontsize=13, y=1.02)\n    plt.tight_layout()\n    plt.show()\nelse:\n    print(\"\\nNeed ≥3 fiscal years for trend chart — run with a larger or more diverse sample.\")\n",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary counts\n",
    "extracted = df_cov[\"Extracted\"].sum()\n",
    "total = len(df_cov)\n",
    "print(f\"Currently extracted: {extracted} / {total} available fields ({100*extracted/total:.0f}%)\")\n",
    "print(f\"\\nFields not yet extracted by source:\")\n",
    "not_extracted = df_cov[~df_cov[\"Extracted\"]]\n",
    "for source, group in not_extracted.groupby(\"Source\"):\n",
    "    print(f\"  {source:<25} {list(group['Data Field'])}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbformat_minor": 5,
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}