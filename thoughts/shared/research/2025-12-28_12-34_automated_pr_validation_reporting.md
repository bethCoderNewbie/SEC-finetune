# Automated PR Validation Reporting Research

**Date:** 2025-12-28
**Project:** SEC Finetune Data Pipeline
**Topic:** Implementing automated validation reporting and PR commenting
**Researcher:** Claude Code Analysis

---

## Executive Summary

This document provides a comprehensive research analysis of two approaches for implementing automated validation reporting and PR commenting:

1. **GitHub Actions (Native)** - Recommended for this project
2. **CML (Continuous Machine Learning)** - Alternative with specific use cases

**Recommendation:** Use GitHub Actions native features with custom markdown generation. CML is better suited for ML-specific metrics; your data preprocessing pipeline benefits from GitHub Actions' simplicity and control.

---

## Current State Analysis

### Validation System Architecture

**Validated Components:**
- Data preprocessing output (C:\Users\bichn\MSBA\SEC finetune\scripts\validation\data_quality\)
- Extraction quality (C:\Users\bichn\MSBA\SEC finetune\scripts\validation\extraction_quality\)
- Feature quality (C:\Users\bichn\MSBA\SEC finetune\scripts\validation\feature_quality\)
- Code quality (C:\Users\bichn\MSBA\SEC finetune\scripts\validation\code_quality\)

**Current Report Output Structure:**
```json
{
  "status": "PASS|WARN|FAIL",
  "timestamp": "ISO-8601",
  "run_directory": "/path/to/run",
  "files_checked": 100,
  "blocking_summary": {
    "total_blocking": 10,
    "passed": 10,
    "failed": 0,
    "warned": 0,
    "all_pass": true
  },
  "validation_table": [
    {
      "category": "extraction_accuracy",
      "metric": "key_item_recall",
      "display_name": "Key Item Recall",
      "target": 0.95,
      "actual": 0.98,
      "status": "PASS",
      "go_no_go": "GO"
    }
  ]
}
```

**Key Classes:**
- `ValidationResult` - Pydantic model for single metric validation (src/config/qa_validation.py:232)
- `HealthCheckValidator` - Runs unified data health checks (src/config/qa_validation.py:593)
- `ThresholdRegistry` - Central threshold definitions (src/config/qa_validation.py:372)
- `ReportFormatter` - Console output formatting (src/utils/reporting.py:6)

**Current Gap:** No markdown report generation exists. Only console and JSON output.

---

## Part 1: GitHub Actions Native Approach

### 1.1 Overview

GitHub Actions provides native capabilities for PR commenting without external dependencies:

**Advantages:**
- No additional service/cost (included in GitHub)
- Fine-grained control over commenting and artifacts
- Works with all GitHub features (checks, status)
- No learning curve for GHA already in use
- Supports matrix builds and conditional logic
- Native artifact upload and retention

**Disadvantages:**
- More boilerplate than CML
- Less turnkey for ML-specific visualizations
- Manual markdown generation required

### 1.2 Key GitHub Actions Tools

#### A. actions/github-script

**Purpose:** Execute JavaScript/Node.js directly in workflow to interact with GitHub API.

**Example Use Case:** Post PR comments with validation results
```yaml
- name: Comment on PR
  if: github.event_name == 'pull_request'
  uses: actions/github-script@v7
  with:
    script: |
      const fs = require('fs');
      const report = JSON.parse(fs.readFileSync('validation_report.json', 'utf8'));

      github.rest.issues.createComment({
        issue_number: context.issue.number,
        owner: context.repo.owner,
        repo: context.repo.repo,
        body: generateMarkdown(report)
      });
```

**Available Context:**
- `github.event` - Full event payload (includes PR details)
- `github.event_name` - Event type (pull_request, push)
- `github.ref` - Branch/tag reference
- `github.run_id` - Workflow run identifier
- `context.issue.number` - PR number (for PR events)

#### B. Upload Artifacts

```yaml
- name: Upload validation reports
  if: always()
  uses: actions/upload-artifact@v4
  with:
    name: validation-reports
    path: |
      validation_report.json
      validation_report.md
    retention-days: 30
```

#### C. Check Runs API

Create detailed check run status on commits:

```yaml
- name: Create check run
  uses: actions/github-script@v7
  with:
    script: |
      github.rest.checks.create({
        owner: context.repo.owner,
        repo: context.repo.repo,
        name: 'Data Validation',
        head_sha: context.sha,
        status: 'completed',
        conclusion: 'success',
        output: {
          title: 'Validation Summary',
          summary: 'All checks passed',
          text: markdownContent
        }
      });
```

### 1.3 Markdown Report Generation

**Implementation in Python:**

```python
from typing import List, Dict, Any

def generate_markdown_report(report: Dict[str, Any]) -> str:
    """Convert JSON validation report to GitHub-flavored markdown."""

    lines = []

    # Header
    lines.append(f"# Validation Report\n")
    lines.append(f"**Status:** {_status_badge(report['status'])}\n")
    lines.append(f"**Timestamp:** {report['timestamp']}\n")

    # Summary Section
    summary = report['blocking_summary']
    lines.append("\n## Summary\n")
    lines.append(f"- Total Checks: {summary['total_blocking']}")
    lines.append(f"- Passed: {_icon('PASS')} {summary['passed']}")
    lines.append(f"- Failed: {_icon('FAIL')} {summary['failed']}")
    lines.append(f"- Warned: {_icon('WARN')} {summary['warned']}\n")

    # Validation Table
    lines.append("\n## Validation Results\n")
    lines.append("| Category | Metric | Actual | Target | Status |")
    lines.append("|----------|--------|--------|--------|--------|")

    for result in report['validation_table']:
        status_icon = _status_icon(result['status'])
        actual = _format_value(result['actual'])
        target = _format_value(result['target'])
        lines.append(
            f"| {result['category']} | {result['metric']} | "
            f"{actual} | {target} | {status_icon} |"
        )

    # Collapsible sections for details
    if len(report['validation_table']) > 10:
        lines.append("\n<details>")
        lines.append("<summary>Full Validation Details</summary>\n")
        # Additional detailed info
        lines.append("</details>\n")

    return "\n".join(lines)


def _status_badge(status: str) -> str:
    """Generate status badge."""
    badges = {
        "PASS": "![PASS](https://img.shields.io/badge/Status-PASS-green)",
        "WARN": "![WARN](https://img.shields.io/badge/Status-WARN-yellow)",
        "FAIL": "![FAIL](https://img.shields.io/badge/Status-FAIL-red)",
    }
    return badges.get(status, "![UNKNOWN](https://img.shields.io/badge/Status-UNKNOWN-gray)")


def _status_icon(status: str) -> str:
    """Get visual icon for status."""
    icons = {
        "PASS": "‚úì PASS",
        "WARN": "‚ö† WARN",
        "FAIL": "‚úó FAIL",
        "SKIP": "‚äò SKIP",
    }
    return icons.get(status, "? UNKNOWN")


def _format_value(value: Any) -> str:
    """Format actual/target values for display."""
    if value is None:
        return "N/A"
    if isinstance(value, float):
        return f"{value:.4f}"
    return str(value)
```

### 1.4 GitHub Actions Workflow Example

Complete workflow for data validation + PR commenting:

```yaml
name: Validation & Reporting

on:
  pull_request:
    branches: [main]
  push:
    branches: [main]

jobs:
  validate-data:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.11"

      - name: Install dependencies
        run: |
          pip install pydantic pydantic-settings
          pip install -e .

      # Run preprocessing validation
      - name: Validate data preprocessing
        id: data-validation
        run: |
          python scripts/validation/data_quality/check_preprocessing_batch.py \
            --run-dir data/processed/latest \
            --output reports/data_validation.json \
            --fail-on-warn || exit_code=$?
          echo "exit_code=${exit_code:-0}" >> $GITHUB_OUTPUT

      # Convert JSON to Markdown
      - name: Generate markdown report
        if: always()
        run: |
          python -c "
          import json
          from pathlib import Path

          # Import your markdown generator
          from src.utils.markdown_reporter import generate_markdown_report

          with open('reports/data_validation.json') as f:
              report = json.load(f)

          markdown = generate_markdown_report(report)
          Path('reports/data_validation.md').write_text(markdown)
          "

      # Upload artifacts
      - name: Upload validation artifacts
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: validation-reports-${{ github.run_number }}
          path: reports/
          retention-days: 30

      # Comment on PR
      - name: Comment validation results on PR
        if: github.event_name == 'pull_request' && always()
        uses: actions/github-script@v7
        with:
          github-token: ${{ secrets.GITHUB_TOKEN }}
          script: |
            const fs = require('fs');
            const path = require('path');

            // Read markdown report
            const reportPath = 'reports/data_validation.md';
            const markdownReport = fs.readFileSync(reportPath, 'utf8');

            // Post comment
            github.rest.issues.createComment({
              issue_number: context.issue.number,
              owner: context.repo.owner,
              repo: context.repo.repo,
              body: markdownReport
            });

      # Create check run with summary
      - name: Create check run
        if: always()
        uses: actions/github-script@v7
        with:
          script: |
            const fs = require('fs');
            const report = JSON.parse(
              fs.readFileSync('reports/data_validation.json', 'utf8')
            );

            const conclusion = report.status === 'PASS' ? 'success'
                            : report.status === 'WARN' ? 'neutral'
                            : 'failure';

            const summary = `
            **Validation Summary**
            - Status: ${report.status}
            - Blocking Checks: ${report.blocking_summary.passed}/${report.blocking_summary.total_blocking} passed
            - Failed: ${report.blocking_summary.failed}
            - Warned: ${report.blocking_summary.warned}
            `;

            github.rest.checks.create({
              owner: context.repo.owner,
              repo: context.repo.repo,
              name: 'Data Validation Report',
              head_sha: context.sha,
              status: 'completed',
              conclusion: conclusion,
              output: {
                title: 'Data Validation Results',
                summary: summary,
                text: fs.readFileSync('reports/data_validation.md', 'utf8')
              }
            });

      # Set workflow status
      - name: Set final status
        if: failure()
        run: exit 1
```

### 1.5 Visual Indicators & Formatting

**Markdown Table with Visual Indicators:**
```
| Status | Description |
|--------|-------------|
| ‚úì PASS | Metric meets or exceeds target |
| ‚ö† WARN | Metric between warn and fail thresholds |
| ‚úó FAIL | Metric below fail threshold |
| ‚äò SKIP | Metric not applicable/measured |
```

**Collapsible Sections (GitHub markdown):**
```markdown
<details>
<summary>Click to expand detailed results</summary>

### Per-File Results
- File1.json: PASS
- File2.json: WARN
- File3.json: PASS

</details>
```

**Status Badges (using shields.io or custom):**
```markdown
![PASS](https://img.shields.io/badge/Status-PASS-green)
![WARN](https://img.shields.io/badge/Status-WARN-yellow)
![FAIL](https://img.shields.io/badge/Status-FAIL-red)
```

---

## Part 2: CML (Continuous Machine Learning) Approach

### 2.1 Overview

CML is a specialized tool by Iterative AI designed for machine learning experiment tracking and reporting.

**Website:** https://cml.dev
**GitHub:** https://github.com/iterative/cml

### 2.2 CML Capabilities

**What CML Does Well:**
- Auto-generated plots and metrics visualizations
- Model performance tracking and comparison
- Integration with DVC (Data Version Control)
- Experiment report generation
- GPU/CPU resource provisioning
- Native PR commenting with formatted tables

**Architecture:**
```
‚îå‚îÄ CML CLI ‚îÄ‚îê
‚îÇ           ‚îú‚îÄ‚Üí Report generation (markdown)
‚îÇ           ‚îú‚îÄ‚Üí Artifact handling
‚îÇ           ‚îú‚îÄ‚Üí PR commenting
‚îÇ           ‚îî‚îÄ‚Üí Metrics plotting
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### 2.3 CML Installation

```bash
# Install via pip
pip install cml

# Or as GitHub Action
uses: iterative/setup-cml@v1

# Docker approach
docker run -it iterative/cml python -m cml.main
```

### 2.4 CML for Non-ML Pipelines

**Compatibility Assessment: PARTIAL**

CML is primarily designed for ML workflows. For pure data preprocessing:

**Can Do:**
- Generate markdown reports from JSON
- Post PR comments
- Store and display metrics over time
- Create comparison plots between runs

**Cannot Do (Easily):**
- Validation-specific visualizations
- Complex threshold logic (built for metrics, not validation rules)
- Integration with existing Python validation classes
- Fine-grained control over report formatting

**Example: Using CML for Data Pipeline**

```yaml
name: CML Report

on:
  pull_request:
    branches: [main]

jobs:
  cml-report:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4

      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.11"

      - name: Install dependencies
        run: |
          pip install cml pydantic pydantic-settings
          pip install -e .

      - name: Run validation
        run: |
          python scripts/validation/data_quality/check_preprocessing_batch.py \
            --run-dir data/processed/latest \
            --output reports/validation.json

      - name: Generate CML report
        run: |
          # Convert JSON to CML-compatible format
          python scripts/utils/cml_reporter.py \
            --input reports/validation.json \
            --output reports/cml_report.md

      - name: Post CML report
        if: github.event_name == 'pull_request'
        run: cml comment create reports/cml_report.md
        env:
          REPO_TOKEN: ${{ secrets.GITHUB_TOKEN }}
```

### 2.5 CML vs GitHub Actions Comparison

| Criterion | CML | GitHub Actions |
|-----------|-----|-----------------|
| **Cost** | Free (open-source) | Free (GitHub included) |
| **Setup** | Medium (new tool) | Low (integrated) |
| **ML-Specific** | Excellent | Generic |
| **Data Pipeline** | Moderate | Excellent |
| **Control** | Limited | Fine-grained |
| **Community** | Large ML focus | Largest (GitHub) |
| **Visualization** | Built-in plots | Markdown only |
| **Threshold Logic** | Simple | Flexible |
| **Customization** | Limited | Unlimited |
| **Learning Curve** | Medium | Low |
| **DVC Integration** | Native | Manual |

---

## Part 3: Report Formatting Best Practices

### 3.1 Markdown Table Formatting

**Example: Validation Results Table**

```markdown
## Validation Results

| Category | Metric | Actual | Target | Diff | Status |
|----------|--------|--------|--------|------|--------|
| **Identity** | CIK Present Rate | 99.8% | 99.0% | +0.8% | ‚úì PASS |
| **Cleanliness** | HTML Artifact Rate | 0.2% | <5% | -4.8% | ‚úì PASS |
| **Substance** | Empty Segment Rate | 0.1% | <2% | -1.9% | ‚úì PASS |
| **Consistency** | Duplicate Rate | 0.0% | <1% | -1.0% | ‚úì PASS |

**Summary:** 4/4 checks passed
```

### 3.2 Visual Indicators

```markdown
Status Icons:
‚úì PASS   - Exceeds or meets target
‚ö† WARN   - Between warn and fail thresholds
‚úó FAIL   - Below fail threshold
‚äò SKIP   - Not applicable/measured

Category Colors (emoji):
üî¥ Critical issues
üü° Warnings/caution
üü¢ Passing
üîµ Informational
```

### 3.3 Collapsible Sections for Large Reports

```markdown
<details>
<summary>üìä Expand for per-file breakdown (156 files)</summary>

### File-Level Results

| File | Status | Issues |
|------|--------|--------|
| filing_001.json | ‚úì PASS | - |
| filing_002.json | ‚ö† WARN | 1 short segment |
| filing_003.json | ‚úì PASS | - |

**Summary by Status:**
- Passed: 150 files
- Warned: 6 files
- Failed: 0 files

</details>
```

### 3.4 Summary Box

```markdown
> **Validation Summary**
> - **Overall Status:** ‚úì PASS
> - **Run Directory:** data/processed/20251228_validation
> - **Files Checked:** 200
> - **Timestamp:** 2025-12-28 15:30:00
> - **Duration:** 45 seconds
```

### 3.5 Progressive Disclosure

For large reports, use markdown structure to hide details:

```markdown
# Validation Report

## üìà Quick Summary
- Status: ‚úì PASS (4/4 blocking checks passed)
- Files: 200 validated
- Time: 45s

## Blocking Checks
[Main table with critical checks]

<details>
<summary>View All Metrics</summary>
[Full validation table]
</details>

<details>
<summary>View Failed/Warned Items</summary>
[Issues breakdown]
</details>
```

---

## Part 4: Implementation Recommendations

### 4.1 For This Project (SEC Finetune)

**Recommended Approach: GitHub Actions + Custom Python**

**Rationale:**
1. Existing validation framework is pure Python (no ML models)
2. Already using GitHub Actions for CI
3. Fine-grained control over report formatting
4. No additional costs or dependencies
5. Validation logic (thresholds, rules) is already sophisticated

**Architecture:**
```
1. Validation Script (Python) ‚Üí JSON Report
   ‚Üì
2. Markdown Generator (Python) ‚Üí Markdown Report
   ‚Üì
3. GitHub Actions Workflow ‚Üí PR Comment + Artifact + Check
```

### 4.2 Implementation Phases

**Phase 1: Markdown Report Generator**
- Create `src/utils/markdown_reporter.py`
- Implement `generate_markdown_report(report: Dict) ‚Üí str`
- Generate visually formatted tables and sections
- Add progress indicators and summary boxes

**Phase 2: Update GitHub Actions Workflow**
- Add validation step after preprocessing
- Generate markdown report from JSON
- Post PR comment with results
- Upload artifacts for retention

**Phase 3: Check Run Integration**
- Create check run on PR for visual status
- Set conclusion based on validation status
- Display summary in PR checks section

**Phase 4: Enhancements (Optional)**
- Add trend tracking (JSON file with historical results)
- Generate comparison reports (this PR vs main)
- Add status badges to README
- Create dashboard view (GitHub Pages)

### 4.3 When to Use CML Instead

Consider CML if:
- You add ML model training to pipeline
- You need built-in experiment comparison
- You want DVC (Data Version Control) integration
- Team already uses CML in other projects
- Need advanced visualization features
- Plan to track metrics over time

---

## Part 5: Code Implementation Examples

### 5.1 Markdown Reporter Module

Location: `src/utils/markdown_reporter.py`

```python
"""Generate GitHub-flavored markdown reports from validation JSON."""

from typing import Any, Dict, List
from datetime import datetime
import json


class MarkdownReporter:
    """Convert JSON validation reports to GitHub markdown."""

    STATUS_ICONS = {
        "PASS": "‚úì",
        "WARN": "‚ö†",
        "FAIL": "‚úó",
        "SKIP": "‚äò",
        "ERROR": "‚ùå",
    }

    STATUS_EMOJI = {
        "PASS": "‚úÖ",
        "WARN": "‚ö†Ô∏è",
        "FAIL": "‚ùå",
        "SKIP": "‚äô",
        "ERROR": "üö®",
    }

    def generate(self, report: Dict[str, Any]) -> str:
        """Generate complete markdown report."""
        sections = [
            self._header(report),
            self._quick_summary(report),
            self._blocking_checks(report),
            self._full_validation_table(report),
            self._detailed_breakdown(report),
        ]
        return "\n\n".join(s for s in sections if s)

    def _header(self, report: Dict) -> str:
        """Generate header with status badge."""
        status = report.get("status", "UNKNOWN")
        status_emoji = self.STATUS_EMOJI.get(status, "‚ùì")

        lines = [
            f"# Validation Report {status_emoji}",
            f"",
            f"**Status:** {self._status_badge(status)}",
        ]

        if "timestamp" in report:
            lines.append(f"**Timestamp:** {report['timestamp']}")

        return "\n".join(lines)

    def _quick_summary(self, report: Dict) -> str:
        """Generate quick summary box."""
        summary = report.get("blocking_summary", {})

        lines = [
            "> **Summary**",
            f"> - Total Blocking Checks: {summary.get('total_blocking', 0)}",
            f"> - {self.STATUS_EMOJI['PASS']} Passed: {summary.get('passed', 0)}",
            f"> - {self.STATUS_EMOJI['FAIL']} Failed: {summary.get('failed', 0)}",
            f"> - {self.STATUS_EMOJI['WARN']} Warned: {summary.get('warned', 0)}",
        ]

        if "files_checked" in report:
            lines.append(f"> - Files Validated: {report['files_checked']}")

        return "\n".join(lines)

    def _blocking_checks(self, report: Dict) -> str:
        """Generate blocking checks table."""
        lines = ["## Blocking Checks Status\n"]

        validation_table = report.get("validation_table", [])
        blocking_results = [
            v for v in validation_table
            if v.get("go_no_go") in ["GO", "NO-GO", "CONDITIONAL"]
        ]

        if not blocking_results:
            return ""

        lines.append("| Status | Metric | Actual | Target |")
        lines.append("|--------|--------|--------|--------|")

        for result in blocking_results:
            status = result.get("status", "SKIP")
            icon = f"{self.STATUS_ICONS[status]}"
            metric = result.get("display_name", result.get("metric", "unknown"))
            actual = self._format_value(result.get("actual"))
            target = self._format_value(result.get("target"))

            lines.append(f"| {icon} {status:4} | {metric} | {actual} | {target} |")

        return "\n".join(lines)

    def _full_validation_table(self, report: Dict) -> str:
        """Generate full validation results table."""
        validation_table = report.get("validation_table", [])

        if not validation_table:
            return ""

        lines = ["## All Validation Results\n"]

        # Only show detailed table if many results
        if len(validation_table) > 10:
            lines.append('<details>')
            lines.append('<summary>Click to expand all results</summary>\n')

        lines.append("| Status | Category | Metric | Actual | Target |")
        lines.append("|--------|----------|--------|--------|--------|")

        for result in validation_table:
            status = result.get("status", "SKIP")
            icon = self.STATUS_ICONS.get(status, "?")
            category = result.get("category", "-")
            metric = result.get("display_name", result.get("metric", "-"))
            actual = self._format_value(result.get("actual"))
            target = self._format_value(result.get("target"))

            lines.append(
                f"| {icon} {status:4} | {category} | {metric} | {actual} | {target} |"
            )

        if len(validation_table) > 10:
            lines.append('\n</details>')

        return "\n".join(lines)

    def _detailed_breakdown(self, report: Dict) -> str:
        """Generate detailed breakdown sections."""
        sections = []

        # Per-file breakdown if available
        if "per_file_results" in report:
            per_file = report["per_file_results"]
            failed_files = [f for f in per_file if f.get("overall_status") == "FAIL"]
            warned_files = [f for f in per_file if f.get("overall_status") == "WARN"]

            if failed_files or warned_files:
                sections.append(self._file_issues_section(failed_files, warned_files))

        return "\n\n".join(sections) if sections else ""

    def _file_issues_section(
        self,
        failed: List[Dict],
        warned: List[Dict]
    ) -> str:
        """Generate section for files with issues."""
        lines = ["## Files with Issues\n"]

        if failed:
            lines.append(f"### Failed Files ({len(failed)})\n")
            for f in failed[:10]:  # Limit to first 10
                lines.append(f"- {f.get('file', 'unknown')}")
            if len(failed) > 10:
                lines.append(f"- ... and {len(failed) - 10} more")
            lines.append("")

        if warned:
            lines.append(f"### Warned Files ({len(warned)})\n")
            for f in warned[:10]:  # Limit to first 10
                lines.append(f"- {f.get('file', 'unknown')}")
            if len(warned) > 10:
                lines.append(f"- ... and {len(warned) - 10} more")

        return "\n".join(lines)

    def _status_badge(self, status: str) -> str:
        """Generate status badge using shields.io."""
        color_map = {
            "PASS": "green",
            "WARN": "yellow",
            "FAIL": "red",
            "ERROR": "red",
        }
        color = color_map.get(status, "gray")
        return (
            f"![{status}]"
            f"(https://img.shields.io/badge/{status}-{color})"
        )

    def _format_value(self, value: Any) -> str:
        """Format value for display."""
        if value is None:
            return "N/A"
        if isinstance(value, bool):
            return "Yes" if value else "No"
        if isinstance(value, float):
            if value > 10:  # Assume percentage or large number
                return f"{value:.2f}"
            return f"{value:.4f}"
        return str(value)


def generate_markdown_report(report: Dict[str, Any]) -> str:
    """Convenience function."""
    reporter = MarkdownReporter()
    return reporter.generate(report)
```

### 5.2 Updated CI Workflow

Location: `.github/workflows/ci.yml`

Add validation and reporting step:

```yaml
  validate-preprocessing:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.11"

      - name: Install dependencies
        run: |
          pip install -e .
          pip install pytest pydantic pydantic-settings

      # Run validation
      - name: Run preprocessing validation
        id: validation
        run: |
          # Find latest preprocessing run directory or use test data
          RUN_DIR=$(ls -dt data/processed/*/ 2>/dev/null | head -1)
          if [ -z "$RUN_DIR" ]; then
            echo "No preprocessing run found, skipping validation"
            echo "status=skipped" >> $GITHUB_OUTPUT
          else
            python scripts/validation/data_quality/check_preprocessing_batch.py \
              --run-dir "$RUN_DIR" \
              --output reports/validation_report.json \
              --fail-on-warn || true
            echo "status=completed" >> $GITHUB_OUTPUT
          fi

      # Generate markdown report
      - name: Generate markdown report
        if: steps.validation.outputs.status == 'completed'
        run: |
          python << 'EOF'
          import json
          from pathlib import Path
          from src.utils.markdown_reporter import generate_markdown_report

          report_path = Path('reports/validation_report.json')
          if report_path.exists():
              with open(report_path) as f:
                  report = json.load(f)

              markdown = generate_markdown_report(report)
              Path('reports/validation_report.md').write_text(markdown)
          EOF

      # Post PR comment
      - name: Comment PR with validation results
        if: github.event_name == 'pull_request' && steps.validation.outputs.status == 'completed'
        uses: actions/github-script@v7
        with:
          script: |
            const fs = require('fs');
            const mdPath = 'reports/validation_report.md';

            if (fs.existsSync(mdPath)) {
              const comment = fs.readFileSync(mdPath, 'utf8');

              github.rest.issues.createComment({
                issue_number: context.issue.number,
                owner: context.repo.owner,
                repo: context.repo.repo,
                body: comment
              });
            }

      # Upload artifacts
      - name: Upload validation reports
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: validation-reports-${{ github.run_id }}
          path: reports/
          retention-days: 30
```

---

## Part 6: Decision Matrix

### For This Project

**GitHub Actions Native:**
- ‚úÖ Already in use
- ‚úÖ Validation framework is pure Python
- ‚úÖ Full control over formatting
- ‚úÖ No additional costs
- ‚úÖ No learning curve
- ‚ùå Manual markdown generation needed
- ‚ùå No built-in visualizations

**CML:**
- ‚ùå Adds external dependency
- ‚ùå Overkill for non-ML pipeline
- ‚ùå Less control over threshold logic
- ‚úÖ Great if ML added later
- ‚úÖ Built-in visualizations
- ‚ö†Ô∏è Medium learning curve

**Recommendation: GitHub Actions + Custom Python Markdown Generator**

---

## Part 7: Quick Start Implementation

### Step 1: Create Markdown Reporter
Save to: `src/utils/markdown_reporter.py` (from Part 5.1)

### Step 2: Add Test
```python
# tests/unit/test_markdown_reporter.py
from src.utils.markdown_reporter import generate_markdown_report

def test_generate_markdown_report():
    report = {
        "status": "PASS",
        "timestamp": "2025-12-28T10:00:00",
        "blocking_summary": {
            "total_blocking": 4,
            "passed": 4,
            "failed": 0,
            "warned": 0,
        },
        "validation_table": [
            {
                "category": "identity",
                "metric": "cik_present_rate",
                "display_name": "CIK Present Rate",
                "actual": 0.998,
                "target": 0.99,
                "status": "PASS",
                "go_no_go": "GO"
            }
        ]
    }

    markdown = generate_markdown_report(report)
    assert "PASS" in markdown
    assert "‚úì" in markdown or "‚úÖ" in markdown
```

### Step 3: Update CI Workflow
Add `validate-preprocessing` job to `.github/workflows/ci.yml` (from Part 5.2)

### Step 4: Test Locally
```bash
python scripts/validation/data_quality/check_preprocessing_batch.py \
  --run-dir data/processed/latest \
  --output /tmp/test_report.json

python << 'EOF'
import json
from src.utils.markdown_reporter import generate_markdown_report

with open('/tmp/test_report.json') as f:
    report = json.load(f)

markdown = generate_markdown_report(report)
print(markdown)
EOF
```

---

## Resources

### GitHub Actions
- [GitHub Actions Documentation](https://docs.github.com/en/actions)
- [actions/github-script](https://github.com/actions/github-script)
- [GitHub API - Create Comment](https://docs.github.com/en/rest/issues/comments#create-an-issue-comment)
- [GitHub Checks API](https://docs.github.com/en/rest/checks)

### CML
- [CML Official Website](https://cml.dev)
- [CML GitHub Repository](https://github.com/iterative/cml)
- [CML Docs](https://cml.dev/doc)
- [CML + DVC Integration](https://cml.dev/doc/usage/dvc)

### Markdown & Formatting
- [GitHub Flavored Markdown](https://github.github.com/gfm/)
- [Shields.io - Status Badges](https://shields.io/)
- [GitHub Markdown Collapsible Sections](https://docs.github.com/en/get-started/writing-on-github/working-with-advanced-formatting)

### Best Practices
- [Keep a Changelog](https://keepachangelog.com/)
- [Conventional Commits](https://www.conventionalcommits.org/)
- [MLOps Community](https://mlops.community/)

---

## Appendix: File Structure for Implementation

```
C:\Users\bichn\MSBA\SEC finetune\
‚îú‚îÄ‚îÄ .github/workflows/
‚îÇ   ‚îî‚îÄ‚îÄ ci.yml                          (UPDATE: add validation job)
‚îÇ
‚îú‚îÄ‚îÄ src/utils/
‚îÇ   ‚îú‚îÄ‚îÄ reporting.py                    (EXISTING: console formatting)
‚îÇ   ‚îî‚îÄ‚îÄ markdown_reporter.py            (NEW: GitHub markdown generation)
‚îÇ
‚îú‚îÄ‚îÄ scripts/validation/
‚îÇ   ‚îú‚îÄ‚îÄ data_quality/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ check_preprocessing_single.py    (EXISTING: outputs JSON)
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ check_preprocessing_batch.py     (EXISTING: outputs JSON)
‚îÇ   ‚îî‚îÄ‚îÄ [other validation scripts]
‚îÇ
‚îú‚îÄ‚îÄ reports/                            (NEW: stores generated reports)
‚îÇ   ‚îú‚îÄ‚îÄ validation_report.json
‚îÇ   ‚îî‚îÄ‚îÄ validation_report.md
‚îÇ
‚îî‚îÄ‚îÄ tests/unit/
    ‚îî‚îÄ‚îÄ test_markdown_reporter.py       (NEW: test reporter)
```

---

## Summary

This research provides:

1. **GitHub Actions native approach** - Recommended (5 stars for this project)
2. **CML alternative** - Better for ML-heavy pipelines (3 stars for this project)
3. **Markdown generation examples** - Production-ready code
4. **Implementation roadmap** - 4-phase approach
5. **Decision matrix** - Clear guidance based on project needs

**Next Steps:**
1. Review this document with team
2. Create markdown reporter module
3. Update GitHub Actions workflow
4. Test with sample validation output
5. Deploy to main branch

