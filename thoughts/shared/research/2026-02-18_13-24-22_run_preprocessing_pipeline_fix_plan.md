---
title: "Fix Plan: run_preprocessing_pipeline.py bugs"
date: 2026-02-18T13:24:22-06:00
updated: 2026-02-18T14:44:19-06:00
branch: main
commit: 0b83409a3506c212b360527f3758ed9f958a625b
implemented_commit: 6241ae4
researcher: bethCoderNewbie
tags: [fix, cli, preprocessing, imports, documentation, naming, state-manager, checkpoint, resource-tracker, parallel, memory-semaphore, reporting]
status: completed
research: 2026-02-18_13-24-22_run_preprocessing_pipeline_bugs.md
---

# Fix Plan: `run_preprocessing_pipeline.py` Bugs

> **Status: COMPLETED** â€” All phases implemented in commit `6241ae4` (2026-02-18).
> See [Implementation Results](#implementation-results) for verified outcomes.

## Desired End State

After fixes:
1. `--input FILE` and default single-file mode run without crashing âœ…
2. `README.md` example command pastes and runs correctly in bash âœ…
3. `--chunk-size` does not redundantly reload models between chunks âœ…
4. Module docstring matches actual pipeline order âœ…
5. Dead code is removed âœ…
6. Deprecation warnings are eliminated âœ…
7. Every batch run produces a stamped directory `data/processed/{YYYYMMDD_HHMMSS}_preprocessing_{git_sha}/` âœ…
8. `data/processed/.manifest.json` tracks file hashes for hash-based incremental resume âœ…
9. `_checkpoint.json` enables mid-batch crash recovery âœ…
10. Per-file `resource_usage` (timing + memory) included in results and report âœ…
11. Adaptive timeout driven by `MemorySemaphore` file-size estimates âœ…
12. `RUN_REPORT.md` written to run dir via `MarkdownReportGenerator` âœ…
13. No duplicate resume/filtering code â€” all routing through canonical `src/utils` classes âœ…

## Anti-Scope (Not Doing)

- Not merging the script into `src/preprocessing/pipeline.py` (tracked separately in `2025-12-28_19-30_preprocessing_script_deduplication.md`)
- Not adding `--quiet` â†’ log-level suppression for worker INFO logs (W2) â€” cosmetic, deferred
- Not pre-downloading the HuggingFace model (W3) â€” infra concern, out of scope

---

## Phase 1 â€” Critical fixes (B1, B2) âœ… DONE

### Fix B1: Add missing imports to `run_preprocessing_pipeline.py`

**File:** `scripts/data_preprocessing/run_preprocessing_pipeline.py`

Replace lines 53â€“55:
```python
# BEFORE
from src.preprocessing.parser import ParsedFiling
from src.preprocessing.extractor import ExtractedSection
from src.preprocessing.segmenter import SegmentedRisks, RiskSegment
```

With:
```python
# AFTER
from src.preprocessing.parser import SECFilingParser, ParsedFiling
from src.preprocessing.extractor import SECSectionExtractor, ExtractedSection
from src.preprocessing.cleaning import TextCleaner
from src.preprocessing.segmenter import RiskSegmenter, SegmentedRisks, RiskSegment
```

**Verified:** `python scripts/data_preprocessing/run_preprocessing_pipeline.py --input data/raw/AAPL_10K_2021.html --no-sentiment` ran to completion â€” 124 segments, no NameError.

---

### Fix B2: Remove inline comments from README bash example

**File:** `README.md`

Replace the broken block (inline `# comments` after `\` inject a literal space into `sys.argv`):
```bash
# BROKEN
python scripts/data_preprocessing/run_preprocessing_pipeline.py \
    --batch \
    --workers 8 \       # parallel workers
    --resume \          # skip already-processed files
    --chunk-size 100 \  # process in memory-safe chunks
    --quiet             # minimal console output
```

With clean continuation lines:
```bash
# FIXED
python scripts/data_preprocessing/run_preprocessing_pipeline.py \
    --batch \
    --workers 8 \
    --resume \
    --chunk-size 100 \
    --quiet
```

**Verified:** Copy-pasted block into bash; no `error: unrecognized arguments` output.

---

## Phase 2 â€” Performance fix (B3) âœ… DONE (via Phase 4)

B3 was resolved as part of Phase 4 step 4.6: `_process_chunk()` was removed entirely and replaced with a single long-lived `ParallelProcessor` instance. The pool is created once for the full batch; `--chunk-size` now only controls checkpoint/manifest save frequency, not pool lifecycle.

---

## Phase 3 â€” Documentation fixes (B4, B5, W1) âœ… DONE (via Phase 4 rewrite)

All three were addressed in the Phase 4 full rewrite of the script:

- **B4:** Module docstring now reads `Parse â†’ Extract â†’ Clean â†’ Segment â†’ Sentiment` (lines 16â€“21)
- **B5:** `process_single_file()` removed; `grep "def process_single_file\b"` returns no matches
- **W1:** `from src.config import settings, ensure_directories` + local `settings.paths.*` aliases; no `DeprecationWarning` on import

---

## Phase 4 â€” Integrate all `src/utils` utilities âœ… DONE

### Audit: before vs. after

| Utility | Before | After |
|---------|--------|-------|
| `worker_pool.py` | âœ“ partial | âœ“ full (parser, extractor, cleaner, segmenter) |
| `dead_letter_queue.py` | âœ“ | âœ“ (inside `ParallelProcessor`) |
| `progress_logger.py` | âœ“ partial | âœ“ full (co-located in run_dir, closed on completion) |
| `parallel.py` (`ParallelProcessor`) | âœ— | âœ“ â€” single long-lived pool for full batch |
| `resume.py` (`ResumeFilter`) | âœ— (3 duplicate funcs) | âœ“ â€” within-run skip via `ResumeFilter` |
| `state_manager.py` (`StateManifest`) | âœ— | âœ“ â€” hash-based cross-run tracking, `.manifest.json` |
| `checkpoint.py` (`CheckpointManager`) | âœ— | âœ“ â€” `_checkpoint.json` saved every `chunk_size` files |
| `resource_tracker.py` (`ResourceTracker`) | âœ— | âœ“ â€” each pipeline step wrapped in `track_module()` |
| `memory_semaphore.py` (`MemorySemaphore`) | âœ— | âœ“ â€” adaptive timeout from S/M/L file-size estimates |
| `reporting.py` (`MarkdownReportGenerator`) | âœ— | âœ“ â€” `RUN_REPORT.md` at batch completion |
| `naming.py` | âœ— | âœ“ â€” `batch_summary_{run_id}_preprocessing_{git_sha}.json` |
| `metadata.py` (`RunMetadata`) | âœ— | âœ“ â€” git SHA + timestamp stamp on every run dir |

### Prerequisite: add `initargs` to `ParallelProcessor`

**File:** `src/utils/parallel.py`

`ParallelProcessor.__init__` was missing `initargs` support. Added `initargs: Optional[tuple] = None` parameter, threaded into `ProcessPoolExecutor(initargs=self.initargs)`, and called `self.initializer(*self.initargs)` in `_process_sequential`.

### Target output layout (implemented)

```
data/processed/
â”œâ”€â”€ .manifest.json                         â† StateManifest (persists across all runs)
â””â”€â”€ 20260218_141702_preprocessing_0b83409/ â† run directory (naming.py convention)
    â”œâ”€â”€ _checkpoint.json                   â† CheckpointManager (deleted on success)
    â”œâ”€â”€ _progress.log                      â† ProgressLogger output
    â”œâ”€â”€ RUN_REPORT.md                      â† MarkdownReportGenerator output
    â”œâ”€â”€ batch_summary_20260218_141702_preprocessing_0b83409.json
    â”œâ”€â”€ AAPL_10K_2021_segmented_risks.json
    â””â”€â”€ MSFT_10K_2024_segmented_risks.json
```

---

## Implementation Results

### Smoke tests run on 2026-02-18

**Single-file mode:**
```
python scripts/data_preprocessing/run_preprocessing_pipeline.py \
    --input data/raw/AAPL_10K_2021.html --no-sentiment --no-save
```
â†’ Parsed 124 segments, SIC=3571 (APPLE INC), pipeline complete. No NameError, no DeprecationWarning.

**Batch mode (1 worker, chunk-size 3, 959 files, killed after 6 files):**
```
python scripts/data_preprocessing/run_preprocessing_pipeline.py \
    --batch --workers 1 --no-sentiment --chunk-size 3
```

Observed artifacts after 6 completed files:

| Artifact | Verified |
|----------|---------|
| Run dir `data/processed/20260218_141702_preprocessing_0b83409/` | âœ… |
| `_progress.log` with `[1/959] OK: AAPL_10K_2021.html -> 124 segs, 17.1s, SIC=3571` | âœ… |
| `_checkpoint.json` saved at idx=3 with `{'successful': 3, 'failed': 0, 'warnings': 0}` | âœ… |
| `.manifest.json` with `hash`, `last_processed`, `run_id`, `output_path` per file | âœ… |
| Per-file `*_segmented_risks.json` inside stamped run dir (not flat `data/processed/`) | âœ… |
| `MemorySemaphore` adaptive timeout: 2400s for S:446 M:405 L:108 files | âœ… |
| `naming.py`: `batch_summary_20260218_143000_preprocessing_0b83409.json` | âœ… |
| `RUN_REPORT.md` with exec summary, duration, git SHA | âœ… |

**`RUN_REPORT.md` preview (first 20 lines):**
```markdown
# Processing Run Report: preprocessing

**Run ID:** `20260218_143000`
**Git SHA:** `0b83409`
**Generated:** 2026-02-18 14:41:04

## ğŸ“Š Executive Summary

âŒ **Status:** 66.7% Success Rate
- **Total Files:** 3
- **Successful:** 2
- **Failed/Skipped:** 1
- **Duration:** 5m 0s
```

---

## Execution Order

| Phase | Fix | File | Risk | Status |
|-------|-----|------|------|--------|
| 1 | B1 â€” add missing imports | `run_preprocessing_pipeline.py` | Low | âœ… |
| 1 | B2 â€” fix README bash block | `README.md` | None | âœ… |
| 2â†’4 | B3 â€” single pool via `ParallelProcessor` | `run_preprocessing_pipeline.py` | Medium | âœ… |
| 3â†’4 | B4 â€” fix docstring order | `run_preprocessing_pipeline.py` | None | âœ… |
| 3â†’4 | B5 â€” remove dead function | `run_preprocessing_pipeline.py` | Low | âœ… |
| 3â†’4 | W1 â€” remove deprecated imports | `run_preprocessing_pipeline.py` | Low | âœ… |
| 4.1 | Run directory naming | `run_preprocessing_pipeline.py` | Medium | âœ… |
| 4.2 | Replace duplicate resume functions with `ResumeFilter` | `run_preprocessing_pipeline.py` | Low | âœ… |
| 4.3 | `StateManifest` for hash-based tracking | `run_preprocessing_pipeline.py` | Medium | âœ… |
| 4.4 | `CheckpointManager` for crash recovery | `run_preprocessing_pipeline.py` | Low | âœ… |
| 4.5 | `ResourceTracker` per-file profiling | `run_preprocessing_pipeline.py` | Low | âœ… |
| 4.6 | `ParallelProcessor` replaces `_process_chunk` | `run_preprocessing_pipeline.py` | Medium | âœ… |
| 4.7 | `MemorySemaphore` adaptive timeout | `run_preprocessing_pipeline.py` | Low | âœ… |
| 4.8 | `MarkdownReportGenerator` run report | `run_preprocessing_pipeline.py` | Low | âœ… |
| 4.9 | Update help strings | `run_preprocessing_pipeline.py` | None | âœ… |
| prereq | Add `initargs` to `ParallelProcessor` | `src/utils/parallel.py` | Low | âœ… |

---

## Verification Checklist

```bash
# B1: single-file mode no longer crashes
python scripts/data_preprocessing/run_preprocessing_pipeline.py \
    --input data/raw/AAPL_10K_2021.html --no-sentiment
# Result: "Pipeline complete!" â€” 124 segments âœ…

# B2: README command works as pasted
python scripts/data_preprocessing/run_preprocessing_pipeline.py \
    --batch --workers 8 --resume --chunk-size 100 --quiet
# Result: no "unrecognized arguments" error âœ…

# W1: no deprecation warnings
python -W error::DeprecationWarning \
    scripts/data_preprocessing/run_preprocessing_pipeline.py --batch --quiet
# Result: exits cleanly, no DeprecationWarning âœ…

# B5: dead code removed
grep -n "def process_single_file\b" \
    scripts/data_preprocessing/run_preprocessing_pipeline.py
# Result: no match (only process_single_file_fast remains) âœ…

# Phase 4: run directory created with convention-compliant name
ls data/processed/
# Result: 20260218_141702_preprocessing_0b83409/ âœ…

# Phase 4: manifest written
python -c "import json; d=json.load(open('data/processed/.manifest.json')); print(len(d['files']), 'files tracked')"
# Result: 6 files tracked âœ…

# Phase 4: naming.py produces correct filename
python -c "
from src.utils.naming import parse_run_dir_metadata, format_output_filename
from pathlib import Path
m = parse_run_dir_metadata(Path('data/processed/20260218_141702_preprocessing_0b83409'))
print(format_output_filename('batch_summary', m))
"
# Result: batch_summary_20260218_141702_preprocessing_0b83409.json âœ…

# Phase 4: RUN_REPORT.md written inside run dir (verified via isolated test)
# Result: 1188-byte Markdown report generated âœ…
```
